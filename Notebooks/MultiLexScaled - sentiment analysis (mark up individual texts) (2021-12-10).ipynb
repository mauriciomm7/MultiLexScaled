{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLexScaled - sentiment analysis (mark up individual texts) (2021-12-10)\n",
    "\n",
    "_by A. Maurits van der Veen_  \n",
    "\n",
    "_Modification history:_  \n",
    "_2021-12-03 - Convert to csv lexica; use newest versions of lexica, as publicly available_  \n",
    "_2021-12-10 - Clean up & streamline for GitHub repo_  \n",
    "\n",
    "This notebook applies sentiment analysis to a small set of texts. It displays these texts in annotated form, showing (either in parentheses or in colour) which words are used in a text's sentiment calculation.\n",
    "\n",
    "The notebook is very helpful to get insights into how MultiLexScaled works and how individual texts are scored. For larger-scale sentiment analysis, please use the `pandas` or `file-based` versions of this notebook (part of the same repo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set-up\n",
    "\n",
    "Specify the main folders containing corpora, notebooks, and code files, as well as the code modules required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAIRfolder = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using python version 3.12.4.\n"
     ]
    }
   ],
   "source": [
    "# Code files to import\n",
    "import sys\n",
    "\n",
    "sys.path.append(STAIRfolder + \"Code\")\n",
    "import os\n",
    "import csv\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sty  # needed for coloured output.\n",
    "# (if not present, install from command line with 'pip install sty', then restart kernel)\n",
    "\n",
    "# local code modules -> these should be in the folder just specified (or otherwise locatable by python)\n",
    "import tokenization\n",
    "import valence\n",
    "import calibrate\n",
    "\n",
    "# Print summary version info (for fuller info, simply print sys.version)\n",
    "print(\"You are using python version {}.\".format(sys.version.split()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify where to find the sentiment analysis lexica and the calibration file, along with their names. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAfolder = STAIRfolder + \"Corpora/Lexica/English/MultiLexScaled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexica = {\n",
    "    \"HuLiu\": SAfolder + \"HuLiu/opinion-lexicon-English/HuLiu_lexiconX.csv\",\n",
    "    \"LabMT_filtered\": SAfolder + \"labMT/labMT_lexicon_filtered.csv\",\n",
    "    \"LexicoderSD\": SAfolder + \"Lexicoder/LSDaug2015/LSD_lexiconX.csv\",\n",
    "    \"MPQA\": SAfolder + \"MPQA 2.0/opinionfinderv2.0/lexicons/MPQA_lexicon.csv\",\n",
    "    \"NRC\": SAfolder + \"NRC/NRC-Emotion-Lexicon-v0.92/NRC_lexicon.csv\",\n",
    "    \"SOCAL\": SAfolder + \"SO-CAL/English (from GitHub)/SO-CAL_lexiconX.csv\",\n",
    "    \"SWN_filtered\": SAfolder + \"SWN/SWN_lexicon_filtered0.1.csv\",\n",
    "    \"WordStat\": SAfolder + \"WordStat/WSD 2.0/WordStat_lexicon2X.csv\",\n",
    "}\n",
    "lexnames = sorted(lexica.keys())\n",
    "\n",
    "# If not using modifiers, just set modifierlex to None\n",
    "modifierlex = SAfolder + \"SO-CAL/English (from GitHub)/SO-CAL_modifiersX.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE this fucntion to load csvs\n",
    "def load_lex(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        cleaned_lines = [line for line in f if line.strip()]\n",
    "\n",
    "    reader = csv.reader(cleaned_lines)\n",
    "    return {row[0].strip(): float(row[1]) for row in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexica & modifier info\n",
    "lexica_used = [load_lex(lexfile) for lexname, lexfile in sorted(lexica.items())]\n",
    "mods = load_lex(modifierlex) if len(modifierlex) > 0 else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the calibration pathname\n",
    "calibrationfolder = SAfolder + \"Calibration/\"\n",
    "calibrationfile = calibrationfolder + \"Calibration_US_2021-12-10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wild = (\n",
    "    \"*\"  # Wildcard character for lexica with wildcard entries (LexicoderSD, WordStat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Specify texts to analyze\n",
    "\n",
    "For small sets of texts, it is easy just to type or copy & paste. Otherwise just read in from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify text or texts to analyze.\n",
    "\n",
    "texts = [\n",
    "    \"He was not hardly happy with the unforeseen unfortunate but not awful outcome\",\n",
    "    \"Things can go from worst to worse to bad to very mediocre to not bad to good to better to best\",\n",
    "    \"While British articles are more negative than American articles when measured against an identical yardstick , the two countries do indeed parallel one another somewhat more closely when each is measured against its own national media landscape\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & pre-tokenize texts (separate out punctuation &c.)\n",
    "\n",
    "texts = [tokenization.punctuationPreprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate valence\n",
    "\n",
    "#### 2.1. Specify parameters\n",
    "\n",
    "We can specify words to ignore (for example, key search terms that might also appear in a valence lexicon), as well as special punctuation to skip (standard punctuation will be skipped automatically). The latter will not be included in the word count; the former will.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignorewords = set()  # Valenced words to ignore, if any, but include in wordcount\n",
    "words2skip = set(\n",
    "    (\".\", \",\", \"...\")\n",
    ")  # Words to skip altogether (usually just punctuation)\n",
    "\n",
    "# Negation words, to combine with modifiers/intensifiers such as 'very' or 'hardly' in adjusting valence\n",
    "negaters = (\n",
    "    \"not\",\n",
    "    \"no\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"nothing\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"nowhere\",\n",
    "    \"noone\",\n",
    "    \"nobody\",\n",
    "    \"lack\",\n",
    "    \"lacked\",\n",
    "    \"lacking\",\n",
    "    \"lacks\",\n",
    "    \"missing\",\n",
    "    \"without\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Valence calculation and mark-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of all keys across our lexica, but remove ignorewords\n",
    "allterms = valence.allkeys(lexica_used)\n",
    "ignoreX = (\n",
    "    set(ignorewords) - allterms\n",
    ")  # Words to skip separately because not in any lexica\n",
    "allterms -= set(ignorewords)  # Update allterms to ignore words in our ignore set\n",
    "\n",
    "# Generate flags indicating whether a lexicon has wildcards\n",
    "wildlexicon = [valence.haswilds(lex) for lex in lexica_used]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate valence data and mark up each text indicating modifier/negation words and valence words\n",
    "results = [\n",
    "    valence.getValence(\n",
    "        text.lower(),\n",
    "        lexica_used,\n",
    "        wildlexicon,\n",
    "        wild,\n",
    "        allterms,\n",
    "        ignoreX,\n",
    "        words2skip,\n",
    "        modifiers=mods,\n",
    "        negaters=negaters,\n",
    "        scaling=\"words\",\n",
    "        flagwords=True,\n",
    "    )\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# Separate results\n",
    "valencedata, markeduptexts = zip(*results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Calibration\n",
    "\n",
    "Our calibration is based on newspaper articles. This has two important implications for valence calculations of short test sentences that may appear counter-intuitive:\n",
    "\n",
    "1. The mean valence of a newspaper article in our representative corpus, as measured by every single one of our lexica, is greater than zero. (This is shown in the list of 'means' displayed when the calibration data are loaded in the next code snippet.) As a result, a sentence that has no valenced words in it at all is (comparatively) negative, rather than neutral, as one might expect.  \n",
    "\n",
    "2. The average newspaper article in our representative corpus is 743 words long. We scale the valence sums encountered by this length, since a single positive word in a 743-word text is far less noticeable than that single positive word by itself. As a result, a 'text' consisting of that single positive word is going to get a strikingly high valence score. To put it differently, the single word 'happy' gets a score of over 63, but so does 'happy happy' and 'happy happy happy ...' ad infinitum. So our valence scores are best interpreted as though the text were repeated back-to-back-to-back up to the average length of a newspaper article. Rather than imagine what the text would look (and 'feel') like repeated back-to-back-to-back, we also divide our calculated valence by the ratio of the length of a test text to the average text length for which the scaler was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory doesn't exist\n"
     ]
    }
   ],
   "source": [
    "# MOVE and Rename Scalers to Calibration\n",
    "os.rename(\n",
    "    \"../Scalers\", r\"../Corpora/Lexica/English/MultiLexScaled/Calibration/\"\n",
    ") if os.path.exists(\"../Scalers\") else print(\"Directory doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor: New scaler for US based on 48283 texts. Generated: 2021-12-10 11:12:39.832747\n",
      "Lexica used (8): ['HuLiu', 'LabMT_filtered', 'LexicoderSD', 'MPQA', 'NRC', 'SOCAL', 'SWN_filtered', 'WordStat']\n",
      "Means: [ 3.33755708e-03  1.89320002e-01  1.21356846e-02  7.69971106e-03\n",
      "  2.26526518e-02  3.71012146e-02  1.27342660e-03 -1.64595405e-04]\n",
      "Std. devs.: [0.01931773 0.08943294 0.02625014 0.01699521 0.02416683 0.05037926\n",
      " 0.00624028 0.03290689]\n",
      "Std. dev. of average across lexica (calculated using 8 lexica): 0.8486667938382454\n"
     ]
    }
   ],
   "source": [
    "# Load calibration data & display some info about it\n",
    "neutralscaler, featurenames, nrfeatures, nravailable, stdev_adj, descriptor = (\n",
    "    calibrate.load_scaler_fromcsv(calibrationfile, includevar=True, displayinfo=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calibration\n",
    "calvalences = calibrate.calibrate_valences(\n",
    "    valencedata, neutralscaler, stdev_adj, firstvalencecol=1, showcomponents=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Display results\n",
    "\n",
    "Here we display each text with a markup indicating intensifiers and valenced words with a value in parentheses immediately following those words (asterisk for intensifiers, and lexicon presence for valenced words), followed by a coloured markup, and concluding with the valence value. Note that some lexica contain the same word with opposite valences. For example, the word `meaning` appears in one lexicon with a negative valence and in another with a positive valence. Unlike the parenthesis-based markup, the coloured markup is based on the net presence in lexica, so `meaning` will appear as though it is not valenced at all.\n",
    "\n",
    "The variable `scaleparam` adjusts the valence calculation by the ratio between a text's length (in words) and this parameter (see the comment at the start of the calibration section above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleparam = 743  # mean nr. words per text in scaler corpus (743 for US rep. corpus)\n",
    "\n",
    "rescaledvalences = [\n",
    "    calval * (len(text.split()) / scaleparam)\n",
    "    for calval, text in zip(calvalences, texts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he was not (*) hardly (*) happy (+8) with the unforeseen (-5) unfortunate (-8) but not (*) awful (-8) outcome (+1) \n",
      "\n",
      "he was \u001b[38;5;89mnot\u001b[39m \u001b[38;5;89mhardly\u001b[39m \u001b[48;2;0;95;0m\u001b[97mhappy\u001b[0m with the \u001b[48;2;0;0;255m\u001b[97munforeseen\u001b[0m \u001b[48;2;0;0;159m\u001b[97munfortunate\u001b[0m but \u001b[38;5;89mnot\u001b[39m \u001b[48;2;0;0;159m\u001b[97mawful\u001b[0m \u001b[48;2;191;255;191moutcome\u001b[0m \n",
      "\n",
      "=> Calibrated valence: -3.61 if taken as full text;\n",
      "                       -0.06 contribution to overall valence of an average-length text\n",
      "\n",
      "\n",
      "things can go from worst (-7) to worse (-8) to bad (-8) to very (*) mediocre (-7) to not (*) bad (-8) to good (+8) to better (+7) to best (+7) \n",
      "\n",
      "things can go from \u001b[48;2;0;0;191m\u001b[97mworst\u001b[0m to \u001b[48;2;0;0;159m\u001b[97mworse\u001b[0m to \u001b[48;2;0;0;159m\u001b[97mbad\u001b[0m to \u001b[38;5;89mvery\u001b[39m \u001b[48;2;0;0;191m\u001b[97mmediocre\u001b[0m to \u001b[38;5;89mnot\u001b[39m \u001b[48;2;0;0;159m\u001b[97mbad\u001b[0m to \u001b[48;2;0;95;0m\u001b[97mgood\u001b[0m to \u001b[48;2;0;127;0m\u001b[97mbetter\u001b[0m to \u001b[48;2;0;127;0m\u001b[97mbest\u001b[0m \n",
      "\n",
      "=> Calibrated valence: -3.29 if taken as full text;\n",
      "                       -0.09 contribution to overall valence of an average-length text\n",
      "\n",
      "\n",
      "while british articles are more (*) negative (-8) than american (+1) articles when measured (+1) against (-4) an (-1) identical yardstick , the two countries do indeed (+1) parallel one another (-1) somewhat (*) more (*) closely (-1,+1) when each is measured (+1) against (-4) its own (+1) national media landscape (+1) \n",
      "\n",
      "while british articles are \u001b[38;5;89mmore\u001b[39m \u001b[48;2;0;0;159m\u001b[97mnegative\u001b[0m than \u001b[48;2;191;255;191mamerican\u001b[0m articles when \u001b[48;2;191;255;191mmeasured\u001b[0m \u001b[48;2;31;31;255m\u001b[97magainst\u001b[0m \u001b[48;2;127;127;255man\u001b[0m identical yardstick , the two countries do \u001b[48;2;191;255;191mindeed\u001b[0m parallel one \u001b[48;2;127;127;255manother\u001b[0m \u001b[38;5;89msomewhat\u001b[39m \u001b[38;5;89mmore\u001b[39m closely when each is \u001b[48;2;191;255;191mmeasured\u001b[0m \u001b[48;2;31;31;255m\u001b[97magainst\u001b[0m its \u001b[48;2;191;255;191mown\u001b[0m national media \u001b[48;2;191;255;191mlandscape\u001b[0m \n",
      "\n",
      "=> Calibrated valence: -3.48 if taken as full text;\n",
      "                       -0.17 contribution to overall valence of an average-length text\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display each sentence twice, first with markup in parentheses and then in color\n",
    "# Follow this by the valence value\n",
    "for markeduptext, calibratedvalence, rescaledvalence in zip(\n",
    "    markeduptexts, calvalences, rescaledvalences\n",
    "):\n",
    "    print(markeduptext, \"\\n\")\n",
    "    print(valence.formatvalencemarking(markeduptext, format=\"color\"), \"\\n\")\n",
    "    print(\n",
    "        \"=> Calibrated valence: {:5.2f} if taken as full text;\\n {:27.2f} contribution to overall valence of an average-length text\\n\\n\".format(\n",
    "            calibratedvalence, rescaledvalence\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blacksmith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
