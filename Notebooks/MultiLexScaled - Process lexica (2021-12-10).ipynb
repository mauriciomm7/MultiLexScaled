{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLexScaled: Process lexica (2021-12-10)\n",
    "\n",
    "_by A. Maurits van der Veen_  \n",
    "\n",
    "_Modification history:_  \n",
    "_2021-11-14 - initial extraction from longer, older notebook_   \n",
    "_2021-12-05 - clean-up and update for Github_   \n",
    "_2021-12-10 - double-check availability of all lexica at URLs indicated_  \n",
    "\n",
    "The sentiment analysis method MultiLexScaled uses 8 widely used, publicly available, lexica. Not all of these are in the same format to start with. This notebook takes the format in which these are available online, and converts each to a csv file.\n",
    "\n",
    "The lexica are processed in the following (alphabetical) order:\n",
    "- HuLiu  \n",
    "- labMT \n",
    "- LexicoderSD  \n",
    "- MPQA  \n",
    "- NRC  \n",
    "- SentiWordNet (filtered0.1)\n",
    "- SO-CAL (including the intensifier dictionary used in valence calculation)\n",
    "- WordStat\n",
    "\n",
    "Most of the lexica receive modifications, which are marked by modifying the filename (either adding `_filtered` or simply a capital `X`:\n",
    "- HuLiu_lexiconX\n",
    "- labMT_lexicon_filtered\n",
    "- LSD_lexiconX\n",
    "- SWN_lexicon_filtered0.1\n",
    "- SO-CAL_lexiconX & SO-CAL_modifiersX\n",
    "- WordStat_lexicon2X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set-up\n",
    "\n",
    "Import code modules and specify project folder path. Also define some useful lexicon-editing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAIRfolder = r\"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using python version 3.11.7.\n"
     ]
    }
   ],
   "source": [
    "# Code files to import\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Print summary version info (for fuller info, simply print sys.version)\n",
    "print(\"You are using python version {}.\".format(sys.version.split()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE dirs to contain all the individual lexicon files\n",
    "os.makedirs(f\"{STAIRfolder}Corpora/Lexica/English/MultiLexScaled/\", exist_ok=True)\n",
    "# Pathname to contain all the individual lexicon files\n",
    "SAfolder = STAIRfolder + \"Corpora/Lexica/English/MultiLexScaled/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lex(lex, fixdict):\n",
    "    \"\"\"Fix a lexicon, by replacing each key in fixdict with the corrected key.\n",
    "\n",
    "    Used to fix apparent unintentional spelling errors in some of the sentiment lexicon keys.\n",
    "    \"\"\"\n",
    "    for oldlexkey, newlexkey in fixdict.items():\n",
    "        if oldlexkey in lex:\n",
    "            lexval = lex[oldlexkey]\n",
    "            del lex[oldlexkey]\n",
    "            if newlexkey != \"\":\n",
    "                lex[newlexkey] = lexval\n",
    "                print(\"Replaced {} by {}\".format(oldlexkey, newlexkey))\n",
    "            else:\n",
    "                print(\"Deleted {}\".format(oldlexkey))\n",
    "\n",
    "    return lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsumed(origx, words, wild=\"*\", report=True):\n",
    "    \"\"\"See whether origx is subsumed by a wildcarded entry in words.\"\"\"\n",
    "    if origx[-1] == wild:\n",
    "        x = origx[:-2] + wild\n",
    "    else:\n",
    "        x = origx + wild\n",
    "    while len(x) > 1:\n",
    "        if x in words:\n",
    "            if report:\n",
    "                print(x, \"subsumes\", origx)\n",
    "            return x\n",
    "        else:\n",
    "            x = x[:-2] + wild\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_removesubsumed(lex):\n",
    "    \"\"\"Remove all entries in a lexicon that are subsumed by a wildcard entry with the same valence.\n",
    "\n",
    "    Note that sometimes these are unintended wildcard matches.\n",
    "    For example: 'terrifi*' (WordStat) is negative because intended for\n",
    "    'terrified', 'terrifies', etc. However, it also subsumes 'terrific*',\n",
    "    which is positive.\n",
    "\n",
    "    In such cases, we want to look for the longest match first, which is\n",
    "    indeed how our wildcard matching function operates. Therefore, we do not\n",
    "    delete such subsumptions.\n",
    "    \"\"\"\n",
    "    entries2delete = []\n",
    "    for key, val in lex.items():\n",
    "        subsumption = subsumed(\n",
    "            key, lex, wild=\"*\", report=True\n",
    "        )  # set report to False for quiet operation\n",
    "        if subsumption:\n",
    "            if lex[subsumption] == val:\n",
    "                entries2delete.append(key)\n",
    "            else:\n",
    "                print(\n",
    "                    \"{} and {} have different valences => keeping both\".format(\n",
    "                        subsumption, key\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if len(entries2delete) > 1:\n",
    "        print(\"Found {} subsumed entries; deleting now\".format(len(entries2delete)))\n",
    "        for entry in entries2delete:\n",
    "            del lex[entry]\n",
    "\n",
    "    return lex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hu & Liu\n",
    "\n",
    "Sentiment lexicon is available here: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html (look for \"opinion lexicon\"). Downloading the lexicon should produce a folder `opinion-lexicon-english` which contains be 2 files: `positive-words.txt` and `negative-words.txt`. Please cite the associated paper:\n",
    "\n",
    "- Minqing Hu and Bing Liu. \"Mining and summarizing customer reviews.\" Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD-2004, full paper), Seattle, Washington, USA, Aug 22-25, 2004. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importHuLiu(lexiconfolder, posfile, negfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import and merge sentiment lexica from Bing Liu\n",
    "\n",
    "    The opinion lexicon rar archive unpacks as a folder opinion-lexicon-English\n",
    "    with the files positive-words.txt and negative-words.txt inside.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    import pickle\n",
    "\n",
    "    words = {}\n",
    "\n",
    "    # Read in the positive words\n",
    "    with open(lexiconfolder + posfile, \"r\", errors=\"ignore\") as infile:\n",
    "        for line in infile.readlines():\n",
    "            if line[0] != \";\" and line.strip() != \"\":\n",
    "                words[line.strip()] = 1\n",
    "    poscount = len(words)\n",
    "\n",
    "    # Read in the negative words\n",
    "    with open(lexiconfolder + negfile, \"r\", errors=\"ignore\") as infile:\n",
    "        for line in infile.readlines():\n",
    "            if line[0] != \";\" and line.strip() != \"\":\n",
    "                words[line.strip()] = -1\n",
    "\n",
    "    # print(list(words.items())[:20])\n",
    "    print(\n",
    "        \"Loaded {} positive and {} negative words, for a total of {} words.\".format(\n",
    "            poscount, len(words) - poscount, len(words)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if savepickle:\n",
    "        with open(lexiconfolder + \"HuLiu_lexicon.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(words, outfile)\n",
    "    if savetext:\n",
    "        with open(lexiconfolder + \"HuLiu_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE Folder if it doesnt exists\n",
    "os.makedirs(f\"{SAfolder}HuLiu/opinion-lexicon-English/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2006 positive and 4780 negative words, for a total of 6786 words.\n"
     ]
    }
   ],
   "source": [
    "lexiconfolder = SAfolder + \"HuLiu/opinion-lexicon-English/\"\n",
    "posfile = \"positive-words.txt\"\n",
    "negfile = \"negative-words.txt\"\n",
    "\n",
    "huliulex = importHuLiu(lexiconfolder, posfile, negfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lexicon has 'bull****' as a euphemism for 'bullshit'. However, our system\n",
    "# would recognize the asterisks as wildcards, so delete this entry.\n",
    "\n",
    "del huliulex[\"bull****\"]\n",
    "\n",
    "# In addition, it has naïve, but the ï does not come through correctly in the loading, so fix that\n",
    "\n",
    "huliulex[\"naïve\"] = huliulex[\"nave\"]\n",
    "del huliulex[\"nave\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the updated file\n",
    "\n",
    "with open(lexiconfolder + \"HuLiu_lexiconX.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(huliulex.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon length: 6785 (2003 positive & 4782 negative)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(huliulex),\n",
    "        sum([1 for x in huliulex if huliulex[x] > 0]),\n",
    "        sum([1 for x in huliulex if huliulex[x] < 0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. labMT (filtered)\n",
    "\n",
    "labMT stands for \"language analysis by Mechanical Turk\". The lexicon is available here: https://github.com/ryanjgallagher/shifterator/tree/master/shifterator/lexicons/labMT (use the file `labMT_English.tsv`). Please cite the associated paper:\n",
    "\n",
    "- Dodds, Peter Sheridan, Kameron Decker Harris, Isabel M. Kloumann, Catherine A. Bliss, and Christopher M. Danforth. \"Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter.\" PLoS ONE 6, no. 12 (2011).\n",
    "\n",
    "labMT is centered around 5, rather than 0. Subtract 5 to get a 0-centered lexicon.\n",
    "\n",
    "Words right around 0 are effectively neutral in valence, so filter out all entries with an absolute value for valence less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE Folder if it doesnt exists\n",
    "os.makedirs(f\"{SAfolder}labMT/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10222"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labMT = {}\n",
    "\n",
    "labMTfolder = SAfolder + \"labMT/\"\n",
    "with open(labMTfolder + \"labMT_English.tsv\", \"r\") as labMTfile:\n",
    "    labreader = csv.reader(labMTfile, delimiter=\"\\t\")\n",
    "    for row in labreader:\n",
    "        labMT[row[0]] = float(row[1]) - 5\n",
    "\n",
    "len(labMT)  # Should be 10,222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3731"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labMTfiltered = {key: val for key, val in labMT.items() if abs(val) >= 1}\n",
    "len(labMTfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(labMTfolder + \"labMT_lexicon_filtered.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(labMTfiltered.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon length: 3731 (2668 positive & 1063 negative)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(labMTfiltered),\n",
    "        sum([1 for x in labMTfiltered if labMTfiltered[x] > 0]),\n",
    "        sum([1 for x in labMTfiltered if labMTfiltered[x] < 0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lexicoder Sentiment Dictionary\n",
    "\n",
    "The Lexicoder Sentiment Dictionary (LSD) was developed by Lori Young and Stuart Soroka. It is available at http://www.snsoroka.com/data-lexicoder/ and is part of the larger Lexicoder system, which also involves text preprocessing as well as some language substitution. Please cite the accompanying paper: \n",
    "\n",
    "- Young, Lori, and Stuart Soroka. \"Affective news: The automated coding of sentiment in political texts.\" Political Communication 29.2 (2012): 205-231.\n",
    "\n",
    "We use the August 2015 version. There are a few multi-word entries in the list; we ignore those. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importLexicoder(\n",
    "    lexfolder,\n",
    "    lexfile,\n",
    "    lexfile_negated,\n",
    "    savepickle=False,\n",
    "    savetext=True,\n",
    "    oldformat=False,\n",
    "):\n",
    "    import pickle\n",
    "\n",
    "    words, negwords = {}, {}\n",
    "\n",
    "    # Read in the non-negated values\n",
    "    curcat = \"\"\n",
    "    curval = 0\n",
    "    with open(lexfolder + lexfile, \"r\") as infile:\n",
    "        for line in infile.readlines():\n",
    "            if oldformat:\n",
    "                line = line.lower()\n",
    "            if line[0] == \"+\" or (oldformat and line[0] != \"\\t\"):\n",
    "                curcat = \"positive\" if \"positive\" in line else \"negative\"\n",
    "                curval = 1 if curcat == \"positive\" else -1\n",
    "            else:\n",
    "                aWord = line.strip().lower()\n",
    "                if oldformat:\n",
    "                    aWord = aWord[:-4]  # remove ' (1)' at end of each line\n",
    "                if \" \" in aWord:\n",
    "                    print(\"Phrase: {} -> skipping\".format(aWord))\n",
    "                    continue\n",
    "                words[aWord] = curval\n",
    "\n",
    "    # Read in the negated values; see which, if any, are new\n",
    "    curcat = \"\"\n",
    "    with open(lexfolder + lexfile_negated, \"r\") as infile:\n",
    "        for line in infile.readlines():\n",
    "            if oldformat:\n",
    "                line = line.lower()\n",
    "            if line[0] == \"+\" or (oldformat and line[0] != \"\\t\"):\n",
    "                curcat = \"positive\" if \"positive\" in line else \"negative\"\n",
    "                curval = 1 if curcat == \"positive\" else -1\n",
    "                continue\n",
    "            linesplit = line.split()\n",
    "            if oldformat:\n",
    "                linesplit = linesplit[:-1]\n",
    "            if linesplit[0] == \"not\" and len(linesplit) == 2:  # skip phrases\n",
    "                aWord = linesplit[1].lower()\n",
    "                if aWord not in words:\n",
    "                    negwords[aWord] = curval\n",
    "                    # print(\"New one from negatives: {} ({})\".format(aWord, curval))\n",
    "                else:\n",
    "                    if words[aWord] != curval:  # should not happen!\n",
    "                        print(\n",
    "                            \"Warning: {} has valence {} in original, but {} in negated file\".format(\n",
    "                                aWord, words[aWord], curval\n",
    "                            )\n",
    "                        )\n",
    "            else:  # phrase not beginning 'not' -> skip\n",
    "                # print('{} = negative file phrase or entry not beginning \"not\": -> skipping'.format(line))\n",
    "                continue\n",
    "\n",
    "    # Identify the ones that are in the basic file but not in the negated one\n",
    "    # print(\"Words in the main file but not the negated file:\")\n",
    "    # print([x for x in words.keys() if x not in negwords])\n",
    "\n",
    "    # Identify the ones that are in the negated list but not the basic one\n",
    "    negwords_pos = [word for word, val in negwords.items() if val == 1]\n",
    "    negwords_neg = [word for word, val in negwords.items() if val == -1]\n",
    "    if len(negwords_pos) > 0:\n",
    "        print(\n",
    "            \"\\nPositive-valence words in the negated file only: {}\".format(\n",
    "                \", \".join(negwords_pos)\n",
    "            )\n",
    "        )\n",
    "    if len(negwords_pos) > 0:\n",
    "        print(\n",
    "            \"\\nNegative-valence words in the negated file only: {}\".format(\n",
    "                \", \".join(negwords_neg)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add them to the full list\n",
    "    for word, val in negwords.items():\n",
    "        if word not in words:\n",
    "            words[word] = val\n",
    "\n",
    "    print(\n",
    "        \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "            len(words),\n",
    "            sum([words[x] for x in words if words[x] == 1]),\n",
    "            -sum([words[x] for x in words if words[x] == -1]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save and return the lexicon\n",
    "    if savepickle:\n",
    "        with open(lexfolder + \"LexicoderDictionary.pkl\", \"wb\") as LSDout:\n",
    "            pickle.dump(words, LSDout)\n",
    "\n",
    "    if savetext:\n",
    "        with open(lexfolder + \"LSD_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE Dirs if they do not doesnt exists\n",
    "os.makedirs(f\"{SAfolder}Lexicoder/LSDaug2015/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: a lie -> skipping\n",
      "Phrase: affected manner* -> skipping\n",
      "Phrase: at odds -> skipping\n",
      "Phrase: back seat -> skipping\n",
      "Phrase: beyond the pale -> skipping\n",
      "Phrase: big lie* -> skipping\n",
      "Phrase: black hole* -> skipping\n",
      "Phrase: black mark* -> skipping\n",
      "Phrase: by a side wind* -> skipping\n",
      "Phrase: can of worms -> skipping\n",
      "Phrase: cast down* -> skipping\n",
      "Phrase: cast off* -> skipping\n",
      "Phrase: cool reception -> skipping\n",
      "Phrase: cool relations -> skipping\n",
      "Phrase: cross the line -> skipping\n",
      "Phrase: cut to pieces* -> skipping\n",
      "Phrase: cut up* -> skipping\n",
      "Phrase: fed up -> skipping\n",
      "Phrase: god knows* -> skipping\n",
      "Phrase: half hearted* -> skipping\n",
      "Phrase: half mast -> skipping\n",
      "Phrase: hang by a thread -> skipping\n",
      "Phrase: hang over* -> skipping\n",
      "Phrase: heart break -> skipping\n",
      "Phrase: heart rending* -> skipping\n",
      "Phrase: heart wounding -> skipping\n",
      "Phrase: heart wrench -> skipping\n",
      "Phrase: holier than thou -> skipping\n",
      "Phrase: hot headed* -> skipping\n",
      "Phrase: hot potato* -> skipping\n",
      "Phrase: hot seat -> skipping\n",
      "Phrase: hot spot* -> skipping\n",
      "Phrase: in hot water -> skipping\n",
      "Phrase: lie to -> skipping\n",
      "Phrase: loop hol* -> skipping\n",
      "Phrase: low end -> skipping\n",
      "Phrase: low expectation -> skipping\n",
      "Phrase: low point -> skipping\n",
      "Phrase: low quality -> skipping\n",
      "Phrase: low rank -> skipping\n",
      "Phrase: low rated -> skipping\n",
      "Phrase: low regard* -> skipping\n",
      "Phrase: mean spirit* -> skipping\n",
      "Phrase: narrow mind* -> skipping\n",
      "Phrase: narrow vision* -> skipping\n",
      "Phrase: no man's land -> skipping\n",
      "Phrase: off balance -> skipping\n",
      "Phrase: off the rails -> skipping\n",
      "Phrase: oh well -> skipping\n",
      "Phrase: out of control -> skipping\n",
      "Phrase: out of pocket -> skipping\n",
      "Phrase: pandora's box -> skipping\n",
      "Phrase: play down -> skipping\n",
      "Phrase: play games -> skipping\n",
      "Phrase: play god -> skipping\n",
      "Phrase: play politics -> skipping\n",
      "Phrase: put off* -> skipping\n",
      "Phrase: rock bottom* -> skipping\n",
      "Phrase: self centred -> skipping\n",
      "Phrase: self conscious* -> skipping\n",
      "Phrase: self importan* -> skipping\n",
      "Phrase: self indulgent -> skipping\n",
      "Phrase: self interest* -> skipping\n",
      "Phrase: self righteous* -> skipping\n",
      "Phrase: self serving -> skipping\n",
      "Phrase: set back -> skipping\n",
      "Phrase: slake on* -> skipping\n",
      "Phrase: slip up -> skipping\n",
      "Phrase: superiority complex -> skipping\n",
      "Phrase: thin ice -> skipping\n",
      "Phrase: tunnel vision* -> skipping\n",
      "Phrase: turn off* -> skipping\n",
      "Phrase: wipe out -> skipping\n",
      "Phrase: wishy washy* -> skipping\n",
      "Phrase: big break* -> skipping\n",
      "Phrase: big heart* -> skipping\n",
      "Phrase: blow a kiss -> skipping\n",
      "Phrase: bound by* -> skipping\n",
      "Phrase: bounden duty* -> skipping\n",
      "Phrase: break no bones -> skipping\n",
      "Phrase: break through -> skipping\n",
      "Phrase: bullet proof -> skipping\n",
      "Phrase: bull's eye -> skipping\n",
      "Phrase: chilled out -> skipping\n",
      "Phrase: clear head* -> skipping\n",
      "Phrase: cloud nine -> skipping\n",
      "Phrase: come to life* -> skipping\n",
      "Phrase: come upon for* -> skipping\n",
      "Phrase: cool head* -> skipping\n",
      "Phrase: cutting edge -> skipping\n",
      "Phrase: dirt cheap* -> skipping\n",
      "Phrase: down to earth* -> skipping\n",
      "Phrase: hard earned -> skipping\n",
      "Phrase: hard fought -> skipping\n",
      "Phrase: hard work* -> skipping\n",
      "Phrase: heart and soul -> skipping\n",
      "Phrase: heart stirring -> skipping\n",
      "Phrase: high end -> skipping\n",
      "Phrase: high expectation* -> skipping\n",
      "Phrase: high point* -> skipping\n",
      "Phrase: high rank -> skipping\n",
      "Phrase: high regard -> skipping\n",
      "Phrase: highly rated -> skipping\n",
      "Phrase: home run* -> skipping\n",
      "Phrase: home stretch -> skipping\n",
      "Phrase: in control -> skipping\n",
      "Phrase: in gear* -> skipping\n",
      "Phrase: in obedience to* -> skipping\n",
      "Phrase: in still water* -> skipping\n",
      "Phrase: indulge in* -> skipping\n",
      "Phrase: keep pace with* -> skipping\n",
      "Phrase: kind word* -> skipping\n",
      "Phrase: laid back -> skipping\n",
      "Phrase: load off -> skipping\n",
      "Phrase: long live -> skipping\n",
      "Phrase: look for* -> skipping\n",
      "Phrase: look to -> skipping\n",
      "Phrase: look up to* -> skipping\n",
      "Phrase: love struck -> skipping\n",
      "Phrase: make head* -> skipping\n",
      "Phrase: mint condition -> skipping\n",
      "Phrase: neck and crop* -> skipping\n",
      "Phrase: occupied with* -> skipping\n",
      "Phrase: of note -> skipping\n",
      "Phrase: on track -> skipping\n",
      "Phrase: one of a kind -> skipping\n",
      "Phrase: patch up -> skipping\n",
      "Phrase: perk up* -> skipping\n",
      "Phrase: play it cool -> skipping\n",
      "Phrase: problem free -> skipping\n",
      "Phrase: problem solv* -> skipping\n",
      "Phrase: red hot -> skipping\n",
      "Phrase: relating to belief* -> skipping\n",
      "Phrase: seize the day -> skipping\n",
      "Phrase: self control -> skipping\n",
      "Phrase: self motivated -> skipping\n",
      "Phrase: set clear* -> skipping\n",
      "Phrase: sky is the limit -> skipping\n",
      "Phrase: smash hit -> skipping\n",
      "Phrase: so kind -> skipping\n",
      "Phrase: stick by -> skipping\n",
      "Phrase: stick it out -> skipping\n",
      "Phrase: stick to it -> skipping\n",
      "Phrase: stick up for -> skipping\n",
      "Phrase: stick with -> skipping\n",
      "Phrase: straight and narrow -> skipping\n",
      "Phrase: straight shooter* -> skipping\n",
      "Phrase: stress free -> skipping\n",
      "Phrase: strike a balance -> skipping\n",
      "Phrase: struck a balance -> skipping\n",
      "Phrase: swear by -> skipping\n",
      "Phrase: top notch* -> skipping\n",
      "Phrase: top rated -> skipping\n",
      "Phrase: under control -> skipping\n",
      "Phrase: walk on air* -> skipping\n",
      "Phrase: without a hitch -> skipping\n",
      "Phrase: worth while* -> skipping\n",
      "Phrase: young at heart -> skipping\n",
      "\n",
      "Positive-valence words in the negated file only: able, clear, commit, communicat*, condon*, cool, credib*, credit*, democratic*, experience, feasib*, fortun*, function*, health*, important, inclin*, influen*, interested*, mix, motivat*, operational*, permit*, plan, play, prepar*, priva*, regard, resistan*, scientific*, social*, spirit*, starter, toleran*, tolerat*, valu*, viab*, will\n",
      "\n",
      "Negative-valence words in the negated file only: pressur*, renewab*, serious*\n",
      "\n",
      "Lexicon length: 4449 (1662 positive & 2787 negative)\n"
     ]
    }
   ],
   "source": [
    "lsdfolder = SAfolder + \"Lexicoder/LSDaug2015/\"\n",
    "lsdfile = \"LSD2015.lc3\"\n",
    "lsdfile_negated = \"LSD2015_NEG.lc3\"\n",
    "\n",
    "lsd_lex = importLexicoder(\n",
    "    lsdfolder, lsdfile, lsdfile_negated, savepickle=False, savetext=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggress* subsumes aggressiv*\n",
      "angr* subsumes angry*\n",
      "apologis* subsumes apologist\n",
      "apologis* and apologist have different valences => keeping both\n",
      "appal* subsumes appall*\n",
      "bare* subsumes barely\n",
      "boast* subsumes boastful*\n",
      "bore* subsumes boredom*\n",
      "confiscat* subsumes confiscation*\n",
      "conspir* subsumes conspira*\n",
      "controvers* subsumes controversy*\n",
      "cram* subsumes cramp\n",
      "cram* subsumes cramped\n",
      "cram* subsumes cramps\n",
      "disagre* subsumes disagree*\n",
      "disagree* subsumes disagreement*\n",
      "disappoint* subsumes disappointment*\n",
      "discord* subsumes discordant*\n",
      "displeas* subsumes displeasur*\n",
      "distres* subsumes distress*\n",
      "distress* subsumes distressing*\n",
      "fals* subsumes falsehood*\n",
      "fals* subsumes falseness*\n",
      "filth* subsumes filthy*\n",
      "fogg* subsumes foggy\n",
      "goddam* subsumes goddamn*\n",
      "grave* subsumes grave\n",
      "grie* subsumes grievous*\n",
      "hothead* subsumes hotheaded*\n",
      "injur* subsumes injury*\n",
      "irascib* subsumes irascibility*\n",
      "irrita* subsumes irritat*\n",
      "nervou* subsumes nervous*\n",
      "oppose* subsumes opposed\n",
      "oppose* subsumes opposer\n",
      "oppose* subsumes opposes\n",
      "pester* subsumes pestering*\n",
      "phobi* subsumes phobic*\n",
      "scari* subsumes scaring\n",
      "spirit* subsumes spiritless\n",
      "spirit* and spiritless have different valences => keeping both\n",
      "strang* subsumes strangl*\n",
      "suffer* subsumes sufferer*\n",
      "sulk* subsumes sulky*\n",
      "troubl* subsumes trouble*\n",
      "worr* subsumes worry*\n",
      "adorn* subsumes adornment\n",
      "affab* subsumes affability*\n",
      "apologis* subsumes apologising\n",
      "authentic* subsumes authenticity*\n",
      "credib* subsumes credibility\n",
      "credib* subsumes credible\n",
      "credit* subsumes credit\n",
      "credit* subsumes creditable\n",
      "credit* subsumes creditably\n",
      "credit* subsumes credited\n",
      "credit* subsumes crediting\n",
      "credit* subsumes credits\n",
      "delight* subsumes delightful*\n",
      "enjoy* subsumes enjoyment*\n",
      "enthus* subsumes enthusiastic*\n",
      "excite* subsumes excited*\n",
      "exult* subsumes exultant*\n",
      "feasib* subsumes feasible\n",
      "fortun* subsumes fortunate*\n",
      "freed* subsumes freedom*\n",
      "gift* subsumes gifted*\n",
      "glee* subsumes gleeful*\n",
      "health* subsumes healthful*\n",
      "health* subsumes healthi*\n",
      "health* subsumes healthy\n",
      "hilari* subsumes hilariou*\n",
      "honest* subsumes honesty*\n",
      "justif* subsumes justify*\n",
      "liberat* subsumes liberation*\n",
      "matur* subsumes maturity*\n",
      "open* subsumes openly\n",
      "open* subsumes openminded*\n",
      "open* subsumes openness\n",
      "pleasant* subsumes pleasantry*\n",
      "pleasur* subsumes pleasurab*\n",
      "positiv* subsumes positivity\n",
      "prepar* subsumes preparatory*\n",
      "spirit* subsumes spirit\n",
      "spirit* subsumes spirited*\n",
      "spirit* subsumes spiriting\n",
      "spirit* subsumes spirits\n",
      "spirit* subsumes spiritual*\n",
      "splend* subsumes splendor*\n",
      "splend* subsumes splendour*\n",
      "straighten* subsumes straightening\n",
      "surviv* subsumes survivor*\n",
      "tribute* subsumes tributes\n",
      "unite* subsumes united*\n",
      "valu* subsumes valuab*\n",
      "viab* subsumes viability*\n",
      "viab* subsumes viabl*\n",
      "victor* subsumes victory*\n",
      "tolera* subsumes toleran*\n",
      "tolera* subsumes tolerat*\n",
      "Found 96 subsumed entries; deleting now\n"
     ]
    }
   ],
   "source": [
    "# Filter out subsumptions (only if they have the same valence)\n",
    "lsd_lex = lex_removesubsumed(lsd_lex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the changed lexicon\n",
    "with open(lsdfolder + \"LSD_lexiconX.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(lsd_lex.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon length: 4353 (1608 positive & 2745 negative)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(lsd_lex),\n",
    "        sum([lsd_lex[x] for x in lsd_lex if lsd_lex[x] == 1]),\n",
    "        -sum([lsd_lex[x] for x in lsd_lex if lsd_lex[x] == -1]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MPQA (Multi-Perspective Question Answering)\n",
    "\n",
    "We use the lexicon associated with OpinionFinder 2.0, available at http://mpqa.cs.pitt.edu/opinionfinder/opinionfinder_2/. (Note that this is slightly different from the subjectivity lexicon available on the same website at: http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/.) We use the field `mpqapolarity`, which has values strongneg, weakneg, weakpos, and strongpos; we translate those values to -1.0, -0.5, 0.5, and 1.0. Two additional values -- neutral and both -- are ignored. If words occur in multiple parts of speech, their valence will be averaged.\n",
    "\n",
    "Please cite the associated paper:\n",
    "\n",
    "- Theresa Wilson, Janyce Wiebe, and Paul Hoffmann (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proc. of HLT-EMNLP-2005. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importMPQA(lexiconfolder, lexiconfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import the length-1 subjectivity clues files from MPQA; convert to dictionary.\n",
    "\n",
    "    Original sentiment assessments are strongpos, weakpos, weakneg, strongneg\n",
    "    Assign values 1, 0.5, -0.5, -1.\n",
    "    Average across different word usages (parts-of-speech)\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    opinionvals = {\n",
    "        \"strongpos\": 1,\n",
    "        \"weakpos\": 0.5,\n",
    "        \"weakneg\": -0.5,\n",
    "        \"strongneg\": -1,\n",
    "        \"neutral\": 0,\n",
    "        \"both\": 0,\n",
    "    }\n",
    "\n",
    "    with open(lexiconfolder + lexiconfile, \"r\") as in1:\n",
    "        cluesdata = in1.readlines()\n",
    "\n",
    "    lexicon = {}\n",
    "    wordcount = 0\n",
    "    for counter, wordinfo in enumerate(cluesdata):\n",
    "        if wordinfo[0] != \"#\":  # skip comment lines (should not be present)\n",
    "            wordsplit = wordinfo.split()\n",
    "\n",
    "            # Skip any terms not of length 1\n",
    "            termlength = [x[-1] for x in wordsplit if x[:3] == \"len\"]\n",
    "            if len(termlength) > 0 and termlength[0] == \"1\":\n",
    "                # Extract word and polarity; these are not always the same location, so search\n",
    "                theword = [x[6:] for x in wordsplit if x[:5] == \"word1\"]\n",
    "                thesent = [x[13:] for x in wordsplit if x[:12] == \"mpqapolarity\"]\n",
    "                if len(theword) > 0 and len(thesent) > 0:\n",
    "                    # Store multiple uses of the same word (varies by POS)\n",
    "                    if theword[0] in lexicon:\n",
    "                        lexicon[theword[0]].append(opinionvals[thesent[0]])\n",
    "                    else:\n",
    "                        lexicon[theword[0]] = [\n",
    "                            opinionvals[thesent[0]],\n",
    "                        ]\n",
    "                        wordcount += 1\n",
    "\n",
    "    # Assign valence by averaging across multiple valences for the same word\n",
    "    # At the same time, filter out words whose average valence is 0\n",
    "    newlexicon = {\n",
    "        key: sum(val) / float(len(val))\n",
    "        for key, val in lexicon.items()\n",
    "        if abs(sum(val)) > 0\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        \"Total lines: %d; unique words: %d, in lexicon: %d\"\n",
    "        % (counter, wordcount, len(newlexicon))\n",
    "    )\n",
    "\n",
    "    if savetext:\n",
    "        with open(lexiconfolder + \"MPQA_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(newlexicon.items())))\n",
    "\n",
    "    if savepickle:\n",
    "        with open(lexiconfolder + \"MPQA_lexicon.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(newlexicon, outfile)\n",
    "\n",
    "    return newlexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE Folder if it doesnt exists\n",
    "os.makedirs(f\"{SAfolder}MPQA 2.0/opinionfinderv2.0/lexicons/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 8220; unique words: 6885, in lexicon: 6449\n"
     ]
    }
   ],
   "source": [
    "mpqafolder = SAfolder + \"MPQA 2.0/opinionfinderv2.0/lexicons/\"\n",
    "mpqafile = \"subjclueslen1polar.tff\"\n",
    "\n",
    "mpqa_lex = importMPQA(mpqafolder, mpqafile, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon length: 6449 (2299 positive & 4150 negative)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(mpqa_lex),\n",
    "        sum([1 for x in mpqa_lex if mpqa_lex[x] > 0]),\n",
    "        sum([1 for x in mpqa_lex if mpqa_lex[x] < 0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NRC (Canadian National Research Council)\n",
    "\n",
    "The NRC lexicon is a LIWC-style lexicon, with values for multiple categorie: each line contains a word, a category, and the value for that category. We use version 0.92 of the lexicon, which is available here: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm (the filename is `NRC-Emotion-Lexicon-Wordlevel-v0.92.txt`). Please cite the associated paper:\n",
    "\n",
    "- Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\n",
    "\n",
    "There are 81 words in the lexicon that are listed as both positive and negative. Through 2021 we included these as positive. In the current version we delete them, in parallel with the way we treat the 'both' options in the MPQA lexicon.\n",
    "\n",
    "There are also 12 words (4 positive, 8 negative) that appear to have been removed from the lexicon since 2015. They are listed at the end of this section for reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importNRC(nrcfolder, nrcfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import NRC sentiment dictionary.\"\"\"\n",
    "    import csv, pickle\n",
    "\n",
    "    words = {}\n",
    "    doublewords = []\n",
    "\n",
    "    with open(nrcfolder + nrcfile, \"r\") as infile:\n",
    "        for row in csv.reader(infile, delimiter=\"\\t\"):\n",
    "            if len(row) > 0 and row[0] != \"\":\n",
    "                word = row[0].strip()\n",
    "                cat = row[1].strip()\n",
    "                val = int(row[2])\n",
    "\n",
    "                if cat == \"negative\" and val == 1:  # should never happen\n",
    "                    if word in words:\n",
    "                        print(\n",
    "                            \"{} was in there already: was {}, now {}.\".format(\n",
    "                                word, words[word], val\n",
    "                            )\n",
    "                        )\n",
    "                    words[word] = -1\n",
    "\n",
    "                elif (\n",
    "                    cat == \"positive\" and val == 1\n",
    "                ):  # happens when word is in list as both negative and positive\n",
    "                    if word in words:\n",
    "                        doublewords.append(word)\n",
    "                        del words[word]\n",
    "                    else:\n",
    "                        words[word] = 1\n",
    "\n",
    "                else:  # other categories: emotions\n",
    "                    pass\n",
    "\n",
    "    print(\n",
    "        \"Words: {}, positive: {}, negative: {}, both (deleted): {}\".format(\n",
    "            len(words),\n",
    "            len([1 for w in words if words[w] == 1]),\n",
    "            len([1 for w in words if words[w] == -1]),\n",
    "            len(doublewords),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if savepickle:\n",
    "        with open(nrcfolder + \"NRC_lexicon.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(words, outfile)\n",
    "\n",
    "    if savetext:\n",
    "        with open(nrcfolder + \"NRC_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words, doublewords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE Folder if it doesnt exists\n",
    "os.makedirs(f\"{SAfolder}NRC/NRC-Emotion-Lexicon-v0.92/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 5462, positive: 2227, negative: 3235, both (deleted): 81\n"
     ]
    }
   ],
   "source": [
    "NRCfolder = SAfolder + \"NRC/NRC-Emotion-Lexicon-v0.92/\"\n",
    "NRCfile = \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "nrclex, posnegwords = importNRC(NRCfolder, NRCfile, savepickle=False, savetext=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon length: 5462 (2227 positive & 3235 negative)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(nrclex),\n",
    "        sum([1 for x in nrclex if nrclex[x] > 0]),\n",
    "        sum([1 for x in nrclex if nrclex[x] < 0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SentiWordNet\n",
    "\n",
    "SentiWordNet is based on the WordNet semantic network. It is available at https://github.com/aesuli/SentiWordNet. Please cite the associated paper:\n",
    "\n",
    "- Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. \"Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining.\" Lrec. Vol. 10. No. 2010. 2010.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE directory if doesn't exists\n",
    "os.makedirs(f\"{SAfolder}SWN/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importSWN(swnfolder, swnfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import SentiWordNet as sentiment analysis lexicon.\n",
    "\n",
    "    Produce a plain sentiment dictionary by separating out members of each synset.\n",
    "    For polysemous words, simply average valences.\n",
    "\n",
    "    File format: tab-separated\n",
    "    - POS (a, n, ...)\n",
    "    - WordNet ID\n",
    "    - positive score (0-1)\n",
    "    - negative score (0-1)\n",
    "    - synset (list of members)\n",
    "    - gloss (verbalization of meaning)\n",
    "\n",
    "    Synset members: space-separated; each word with the suffix '#n' where n is the sequence\n",
    "    number for the synset memberships of the same word.\n",
    "\n",
    "    Ignore multi-word phrases. The dataset contains 83499 words, of which 20,099 positive,\n",
    "    20,698 negative (but 9,783 of the positive/negative words are both). If we sum the positive\n",
    "    and negative, there are 29,502 words with a non-zero sum.\n",
    "\n",
    "    There is commented-out code in here for an alternative way of generating a lexicon.\n",
    "    Un-comment if needed. The alterantive way is to ignore all the zero-values assigned\n",
    "    to a word's positive or negative scores and include the non-zero values only. These\n",
    "    data are tracked in the variable nonzerovals and compiled into the lexicon polarity2.\n",
    "    \"\"\"\n",
    "    import csv, pickle\n",
    "    from operator import itemgetter\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    synsets, wordpos, posvals, negvals, nonzerovals, polarity, polarity2 = (\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "        {},\n",
    "    )\n",
    "\n",
    "    with open(swnfolder + swnfile, \"r\") as infile:\n",
    "        # Run down synsets\n",
    "        for row in csv.reader(infile, delimiter=\"\\t\"):\n",
    "            if len(row) > 0 and row[0] != \"\" and row[0][0] != \"#\":  # skip comments &c.\n",
    "                pos = row[0].strip()\n",
    "                id = row[1].strip()\n",
    "                posval = float(row[2])\n",
    "                negval = float(row[3])\n",
    "                synsetraw = row[4].split()\n",
    "\n",
    "                # Run down members of each synset\n",
    "                for term in synsetraw:\n",
    "                    word = term[:-2]  # skip the synset number (#1, #2, etc.)\n",
    "                    if \"_\" not in word:  # ignore multi-word phrases\n",
    "                        # Track non-zero values\n",
    "                        # if posval != 0:\n",
    "                        #     if word in nonzerovals:\n",
    "                        #         nonzerovals[word].append(posval)\n",
    "                        #     else:\n",
    "                        #         nonzerovals[word] = [posval,]\n",
    "                        # if negval != 0:\n",
    "                        #     if word in nonzerovals:\n",
    "                        #         nonzerovals[word].append(-negval)\n",
    "                        #     else:\n",
    "                        #         nonzerovals[word] = [-negval,]\n",
    "\n",
    "                        # Weight zero-values as equal to non-zero values\n",
    "                        if word not in synsets:  # first or only meaning of word\n",
    "                            synsets[word] = [\n",
    "                                id,\n",
    "                            ]\n",
    "                            wordpos[word] = [\n",
    "                                pos,\n",
    "                            ]\n",
    "                            posvals[word] = posval\n",
    "                            negvals[word] = negval\n",
    "\n",
    "                        else:  # polysemous word\n",
    "                            nrmeanings = len(synsets[word])\n",
    "                            oldposvals = posvals[word] * nrmeanings\n",
    "                            oldnegvals = negvals[word] * nrmeanings\n",
    "                            synsets[word].append(id)\n",
    "                            if pos not in wordpos[word]:\n",
    "                                wordpos[word].append(pos)\n",
    "                            posvals[word] = (oldposvals + posval) / float(\n",
    "                                nrmeanings + 1\n",
    "                            )\n",
    "                            negvals[word] = (oldnegvals + negval) / float(\n",
    "                                nrmeanings + 1\n",
    "                            )\n",
    "\n",
    "    # Combine positive and negative\n",
    "    # Note: this treats 0 values as equal in weight to non-zero values\n",
    "    for word, val in posvals.items():\n",
    "        sumvals = val - negvals[word]\n",
    "        if sumvals != 0:\n",
    "            polarity[word] = sumvals\n",
    "\n",
    "    # We could also focus on non-zero values only:\n",
    "    # for word, vals in nonzerovals.items():\n",
    "    #     avgval = sum(vals)/len(vals)\n",
    "    #     if avgval != 0:\n",
    "    #         polarity2[word] = avgval\n",
    "\n",
    "    # Report basic data\n",
    "    print(\n",
    "        \"Words: {}, positive: {}, negative: {}, both: {}, sum non-0: {}\".format(\n",
    "            len(synsets),\n",
    "            len([1 for w in posvals if posvals[w] > 0]),\n",
    "            len([1 for w in negvals if negvals[w] > 0]),\n",
    "            len([1 for w in posvals if posvals[w] > 0 and negvals[w] > 0]),\n",
    "            len(polarity),\n",
    "            # len(polarity2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if savepickle:\n",
    "        with open(swnfolder + \"SWNlex.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump((polarity, posvals, negvals, synsets, wordpos), outFile)\n",
    "\n",
    "    if savetext:\n",
    "        with open(swnfolder + \"SWN_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(polarity.items())))\n",
    "        # with open(swnfolder + 'SWN_lexicon2.csv', 'w') as outfile:\n",
    "        #     outwriter = csv.writer(outfile)\n",
    "        #     outwriter.writerows(sorted(list(polarity2.items())))\n",
    "\n",
    "    return polarity  # , polarity2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 83499, positive: 20099, negative: 20698, both: 9783, sum non-0: 29502\n"
     ]
    }
   ],
   "source": [
    "SWNfolder = SAfolder + \"SWN/\"\n",
    "SWNfile = \"SentiWordNet_3.0.0.txt\"\n",
    "\n",
    "# swnlex, swnlex2 = importSWN(SWNfolder, SWNfile, savepickle=False, savetext=True)\n",
    "swnlex = importSWN(SWNfolder, SWNfile, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24222"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out all the values that are very close to 0 (use 0.1 as the cut-off)\n",
    "\n",
    "cutoff = 0.1\n",
    "swn_filtered = {key: val for key, val in swnlex.items() if abs(val) >= cutoff}\n",
    "len(swn_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated version\n",
    "with open(SWNfolder + \"SWN_lexicon_filtered0.1.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(swn_filtered.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lexicon length: 24222 (11116 positive & 13106 negative)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(swn_filtered),\n",
    "        sum([1 for x in swn_filtered if swn_filtered[x] > 0]),\n",
    "        sum([1 for x in swn_filtered if swn_filtered[x] < 0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SO-CAL\n",
    "\n",
    "The SO-CAL dictionaries are available at https://github.com/sfu-discourse-lab/SO-CAL/tree/master/Resources/dictionaries/English. Please cite the associated paper:\n",
    "\n",
    "- Taboada, Maite, Julian Brooke, Milan Tofiloski, Kimberly Voll and Manfred Stede (2011) Lexicon-Based Methods for Sentiment Analysis. Computational Linguistics 37 (2): 267-307.\n",
    "\n",
    "Note: SO-CAL's dictionaries provide only the singular for nouns and the infinitive for verbs. We expand those lists by adding plurals as well as basic verb conjugations. For this, we use the python module `pattern` ( https://github.com/clips/pattern).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD pattern module to NBs dir\n",
    "MODULE = \"/pattern\"\n",
    "sys.path.append(MODULE)\n",
    "import pattern.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importSOCAL(socalfolder, socalnames, savepickle=False, savetext=True):\n",
    "    \"\"\"Import and pre-process SO-CAL sentiment lexicon\n",
    "\n",
    "    Expects 5 separate files in the socalfolder, with names specified in the\n",
    "    dictionary socalnames, which should have the keys 'adjective', 'adverb',\n",
    "    'noun', and 'verb', plus 'intensifier' for the separate intensifiers list.\n",
    "\n",
    "    We ignore part-of-speech, and we expand nouns (pluralize) and verbs\n",
    "    (singular, plural, prsent, past, etc.) to combine into a single dictionary.\n",
    "    We use the pattern module for these expansions.\n",
    "    When words appear in multiple sub-lists, simply average their valence.\n",
    "\n",
    "    Each file contains 1 word per line, tab-separated from its valence.\n",
    "    For the 4 POS categories, these are words to be added to the lexicon; for the last,\n",
    "    the 'valence' is the multiplier to use when the word precedes a valence\n",
    "    word.\n",
    "\n",
    "    Note: A number of these 'words' are multi-word phrases, some with\n",
    "    generic wildcards and POS tags (e.g. \"(bowl)_#PER?#_over\" ). These are all separated\n",
    "    by hyphens or underscores; skip them here, to keep single words only in the lexicon.\n",
    "\n",
    "    Note 2: some entries are in the verb dictionary twice, and their valence is automatically averaged\n",
    "    - ameliorate, at 1 and 2\n",
    "    - appall, at -3 and -5\n",
    "    - befriend (both at 1)\n",
    "    - belie, at -2 and -3\n",
    "    - bug, at -1 and -2\n",
    "    - enthrall (both at 3)\n",
    "    - extol, at 2 and 3\n",
    "    - gladden, at 2 and 3\n",
    "    - loathe at -4 and loath at -5\n",
    "    - misunderstand (both at -1), plus misunderstood, also at -1\n",
    "    - quibble (both at -1)\n",
    "    - uplift, at 2 and 3\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    from operator import itemgetter\n",
    "    from pattern.en import pluralize\n",
    "    import pickle\n",
    "\n",
    "    words, counts = {}, {}\n",
    "\n",
    "    # Read in the nouns; handle plurals\n",
    "    counter = 0\n",
    "    with open(socalfolder + socalnames[\"noun\"], \"r\", errors=\"ignore\") as infile:\n",
    "        inreader = csv.reader(infile, delimiter=\"\\t\")\n",
    "        for row in inreader:\n",
    "            if len(row) == 2:  # only take lines with 2 entries\n",
    "                counter += 1\n",
    "                theword = row[0]\n",
    "                theval = int(row[1])\n",
    "                words, counts = updatewordscounts(theword, theval, words, counts)\n",
    "                words, counts = updatewordscounts(\n",
    "                    pluralize(theword), theval, words, counts\n",
    "                )\n",
    "    lensofar = len(words)\n",
    "    print(\n",
    "        \"Processed {} nouns; total count (singular + plural): {}\".format(\n",
    "            counter, lensofar\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Read in the verbs; handle conjugation\n",
    "    with open(socalfolder + socalnames[\"verb\"], \"r\") as infile:\n",
    "        inreader = csv.reader(infile, delimiter=\"\\t\")\n",
    "        for row in inreader:\n",
    "            if len(row) == 2:\n",
    "                theval = int(row[1])\n",
    "                for verbtense in alltenses(row[0]):\n",
    "                    words, counts = updatewordscounts(verbtense, theval, words, counts)\n",
    "    print(\"Verbs (incl. conjugations):\", len(words) - lensofar)\n",
    "    lensofar = len(words)\n",
    "\n",
    "    # Read in adjectives & adverbs: no special treatment\n",
    "    for pos in (\"adjective\", \"adverb\"):\n",
    "        with open(socalfolder + socalnames[pos], \"r\", errors=\"ignore\") as infile:\n",
    "            inreader = csv.reader(infile, delimiter=\"\\t\")\n",
    "            for row in inreader:\n",
    "                if len(row) == 2:\n",
    "                    words, counts = updatewordscounts(\n",
    "                        row[0], int(row[1]), words, counts\n",
    "                    )\n",
    "    print(\"Adjectives & adverbs:\", len(words) - lensofar)\n",
    "\n",
    "    # Remove phrases, to keep single words only\n",
    "    terms2delete = []\n",
    "    for term, val in words.items():\n",
    "        if \"_\" in term:  # redundant: or '(' in term or '[' in term\n",
    "            terms2delete.append(term)\n",
    "            # print(term)\n",
    "    print(\"Deleting {} phrases (identified by underscores)\".format(len(terms2delete)))\n",
    "    for term in terms2delete:\n",
    "        del words[term]\n",
    "\n",
    "    print(\"Total lexicon length:\", len(words))\n",
    "\n",
    "    # Now read in the modifiers\n",
    "    mods = {}\n",
    "    with open(socalfolder + socalnames[\"intensifier\"], \"r\") as infile:\n",
    "        inreader = csv.reader(infile, delimiter=\"\\t\")\n",
    "        for row in inreader:\n",
    "            if len(row) == 2:\n",
    "                mods[row[0]] = float(row[1])\n",
    "    print(\"Modifiers:\", len(mods))\n",
    "\n",
    "    # Print some output to double-check everything looks right\n",
    "    # print(list(words.items())[:20])\n",
    "    # print(\"Total nr. of words\", len(words))\n",
    "    # print(list(mods.items())[:20])\n",
    "    # print(\"Total nr. of modifiers\", len(mods))\n",
    "    # dupes = [(word, val) for word, val in counts.items() if val > 1]\n",
    "    # print(sorted(dupes, key=itemgetter(1), reverse=True))\n",
    "    # print(\"Total nr. of words encountered more than once\", len(dupes))\n",
    "\n",
    "    # Save SO-CAL dictionary in a text or pickle file\n",
    "    if savetext:\n",
    "        with open(socalfolder + \"SO-CAL_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "        with open(socalfolder + \"SO-CAL_modifiers.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(mods.items())))\n",
    "    if savepickle:\n",
    "        with open(socalfolder + \"SO-CAL.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump((words, mods, counts), outfile)\n",
    "\n",
    "    return words, mods\n",
    "\n",
    "\n",
    "def updatewordscounts(word, val, words, counts):\n",
    "    \"\"\"Update words & counts dictionaries\"\"\"\n",
    "    if word != \"\":\n",
    "        if word in words:\n",
    "            words[word] = words[word] * counts[word] + val\n",
    "            counts[word] += 1\n",
    "            words[word] /= float(counts[word])\n",
    "        else:\n",
    "            words[word] = val\n",
    "            counts[word] = 1\n",
    "    return words, counts\n",
    "\n",
    "\n",
    "def alltenses(v):\n",
    "    \"\"\"Return all different verb forms, given infinitive.\n",
    "\n",
    "    Uses the Linguistics module from Nodebox (called 'en').\n",
    "    \"\"\"\n",
    "    from pattern.en import conjugate\n",
    "\n",
    "    # filter out multi-word phrases, which we just ignore\n",
    "    if \"_\" in v:\n",
    "        return []\n",
    "    return list(\n",
    "        set(\n",
    "            [\n",
    "                conjugate(v, conj)\n",
    "                for conj in [\n",
    "                    \"inf\",\n",
    "                    \"1sg\",\n",
    "                    \"2sg\",\n",
    "                    \"3sg\",\n",
    "                    \"pl\",\n",
    "                    \"part\",\n",
    "                    \"1sgp\",\n",
    "                    \"2sgp\",\n",
    "                    \"3sgp\",\n",
    "                    \"ppl\",\n",
    "                    \"ppart\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{SAfolder}SO-CAL/English (from GitHub)/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1549 nouns; total count (singular + plural): 3070\n",
      "Verbs (incl. conjugations): 3562\n",
      "Adjectives & adverbs: 3422\n",
      "Deleting 29 phrases (identified by underscores)\n",
      "Total lexicon length: 10025\n",
      "Modifiers: 210\n"
     ]
    }
   ],
   "source": [
    "# Load the SO-CAL lexicon from the official distribution, version 1.11.\n",
    "\n",
    "# Note: the pattern libraries conjugate function may raise an error\n",
    "# (StopIteration) the first time this is run. Just execute the cell again.\n",
    "\n",
    "socalfolder = SAfolder + \"SO-CAL/English (from GitHub)/\"\n",
    "socalsuffix = \"_dictionary1.11.txt\"\n",
    "socalabbrevs = [\"noun\", \"verb\", \"adj\", \"adv\", \"int\"]\n",
    "socalcats = [\"noun\", \"verb\", \"adjective\", \"adverb\", \"intensifier\"]\n",
    "socalnames = {cat: abbrev + socalsuffix for cat, abbrev in zip(socalcats, socalabbrevs)}\n",
    "\n",
    "socallex, socalmods = importSOCAL(socalfolder, socalnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting bad version of cliché\n",
      "Deleting bad \"scsl...\" term\n"
     ]
    }
   ],
   "source": [
    "# There are 2 entries with an accented letter that do not come through correctly:\n",
    "# cliché and 'scslsiscshs�s s'. The latter can be deleted (not sure what it is supposed to be).\n",
    "# The former should be corrected. Relatedly, there is an entry for clichd, which should be clichéd.\n",
    "\n",
    "badkey = \"\"\n",
    "for key, val in socallex.items():\n",
    "    if key[:5] == \"clich\" and \"e\" not in key and \"d\" not in key:\n",
    "        badkey = key\n",
    "if len(badkey) > 0:\n",
    "    print(\"Deleting bad version of cliché\")\n",
    "    del socallex[badkey]\n",
    "\n",
    "badkey = \"\"\n",
    "for key, val in socallex.items():\n",
    "    if key[:4] == \"scsl\":\n",
    "        badkey = key\n",
    "if len(badkey) > 0:\n",
    "    print('Deleting bad \"scsl...\" term')\n",
    "    del socallex[badkey]\n",
    "\n",
    "socallex[\"cliché\"] = -2  # Same as for 'cliche'\n",
    "socallex[\"clichés\"] = -2  # Same as for 'cliches'\n",
    "\n",
    "if \"clichd\" in socallex:\n",
    "    del socallex[\"clichd\"]\n",
    "socallex[\"clichéd\"] = -3  # Same as for 'cliched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify some additional fixes to the SO-CAL lexicon\n",
    "# Mostly these fix spelling errors\n",
    "\n",
    "socallex_fix = {  # adjectives\n",
    "    \"redepemption\": \"\",\n",
    "    \"suspensful\": \"\",\n",
    "    \"uncoventional\": \"\",\n",
    "    \"anti-climatic\": \"anti-climactic\",\n",
    "    \"autorcratic\": \"autocratic\",\n",
    "    \"devestating\": \"devastating\",\n",
    "    \"digruntled\": \"disgruntled\",\n",
    "    \"disasterous\": \"disastrous\",\n",
    "    \"forlon\": \"forlorn\",\n",
    "    \"futurisitic\": \"futuristic\",\n",
    "    \"inprudent\": \"imprudent\",\n",
    "    \"intractible\": \"intractable\",\n",
    "    \"juandiced\": \"jaundiced\",\n",
    "    \"less-than-desireable\": \"less-than-desirable\",\n",
    "    \"obsure\": \"obscure\",\n",
    "    \"opressive\": \"oppressive\",\n",
    "    \"plebian\": \"plebeian\",\n",
    "    \"priviledged\": \"privileged\",\n",
    "    \"pgnacious\": \"pugnacious\",\n",
    "    \"strenous\": \"strenuous\",\n",
    "    \"uneveness\": \"unevenness\",\n",
    "    \"unweildy\": \"unwieldy\",\n",
    "    \"uproductive\": \"unproductive\",\n",
    "    \"inpudent\": \"impudent\",\n",
    "    # adverbs\n",
    "    \"immensly\": \"immensely\",  # this overwrites 'immensely' that was in there at valence 1\n",
    "    \"exceptionaly\": \"\",  # just delete\n",
    "    \"digitaly\": \"\",  # ,,\n",
    "    \"entirly\": \"entirely\",\n",
    "    \"realy\": \"really\",\n",
    "    \"disasterously\": \"disastrously\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted redepemption\n",
      "Deleted suspensful\n",
      "Deleted uncoventional\n",
      "Replaced anti-climatic by anti-climactic\n",
      "Replaced autorcratic by autocratic\n",
      "Replaced devestating by devastating\n",
      "Replaced digruntled by disgruntled\n",
      "Replaced disasterous by disastrous\n",
      "Replaced forlon by forlorn\n",
      "Replaced futurisitic by futuristic\n",
      "Replaced inprudent by imprudent\n",
      "Replaced intractible by intractable\n",
      "Replaced juandiced by jaundiced\n",
      "Replaced less-than-desireable by less-than-desirable\n",
      "Replaced obsure by obscure\n",
      "Replaced opressive by oppressive\n",
      "Replaced plebian by plebeian\n",
      "Replaced priviledged by privileged\n",
      "Replaced pgnacious by pugnacious\n",
      "Replaced strenous by strenuous\n",
      "Replaced uneveness by unevenness\n",
      "Replaced unweildy by unwieldy\n",
      "Replaced uproductive by unproductive\n",
      "Replaced inpudent by impudent\n",
      "Replaced immensly by immensely\n",
      "Deleted exceptionaly\n",
      "Deleted digitaly\n",
      "Replaced entirly by entirely\n",
      "Replaced realy by really\n",
      "Replaced disasterously by disastrously\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'masterpiece': 5,\n",
       " 'masterpieces': 5,\n",
       " 'perfection': 5,\n",
       " 'perfections': 5,\n",
       " 'classic': 3.5,\n",
       " 'classics': 4,\n",
       " 'mastery': 4,\n",
       " 'masteries': 4,\n",
       " 'wonder': 4,\n",
       " 'wonders': 4,\n",
       " 'zenith': 4,\n",
       " 'zeniths': 4,\n",
       " 'beauty': 4,\n",
       " 'beauties': 4,\n",
       " 'bliss': 4,\n",
       " 'blisses': 4,\n",
       " 'brilliance': 4,\n",
       " 'brilliances': 4,\n",
       " 'culmination': 4,\n",
       " 'culminations': 4,\n",
       " 'ecstasy': 4,\n",
       " 'ecstasies': 4,\n",
       " 'excellence': 4,\n",
       " 'excellences': 4,\n",
       " 'exhilaration': 4,\n",
       " 'exhilarations': 4,\n",
       " 'genius': 4,\n",
       " 'genii': 4,\n",
       " 'magnificence': 4,\n",
       " 'magnificences': 4,\n",
       " 'tour-de-force': 4,\n",
       " 'tour-de-forces': 4,\n",
       " 'pinnacle': 4,\n",
       " 'pinnacles': 4,\n",
       " 'revelation': 4,\n",
       " 'revelations': 4,\n",
       " 'jubilation': 4,\n",
       " 'jubilations': 4,\n",
       " 'euphoria': 4,\n",
       " 'euphorias': 4,\n",
       " 'elation': 3.0,\n",
       " 'elations': 3.0,\n",
       " 'stud': 3,\n",
       " 'studs': 3,\n",
       " 'angel': 3,\n",
       " 'angels': 3,\n",
       " 'freshness': 3,\n",
       " 'freshnesses': 3,\n",
       " 'gaiety': 3,\n",
       " 'gaieties': 3,\n",
       " 'heroism': 3,\n",
       " 'heroisms': 3,\n",
       " 'honor': 2.5,\n",
       " 'honors': 2.5,\n",
       " 'intelligence': 3,\n",
       " 'intelligences': 3,\n",
       " 'love': 3.0,\n",
       " 'profoundness': 3,\n",
       " 'profoundnesses': 3,\n",
       " 'thoughtfulness': 3,\n",
       " 'thoughtfulnesses': 3,\n",
       " 'valor': 3,\n",
       " 'valors': 3,\n",
       " 'wisdom': 3,\n",
       " 'wisdoms': 3,\n",
       " 'accomplishment': 3,\n",
       " 'accomplishments': 3,\n",
       " 'allure': 3,\n",
       " 'allures': 3,\n",
       " 'affection': 3,\n",
       " 'affections': 3,\n",
       " 'admiration': 3,\n",
       " 'admirations': 3,\n",
       " 'achievement': 3,\n",
       " 'achievements': 3,\n",
       " 'asset': 3,\n",
       " 'assets': 3,\n",
       " 'awe': 3.5,\n",
       " 'awes': 3.5,\n",
       " 'benefit': 3,\n",
       " 'benefits': 3,\n",
       " 'boldness': 3,\n",
       " 'boldnesses': 3,\n",
       " 'bounty': 3,\n",
       " 'bounties': 3,\n",
       " 'bravery': 3,\n",
       " 'braveries': 3,\n",
       " 'brightness': 3,\n",
       " 'brightnesses': 3,\n",
       " 'champion': 3,\n",
       " 'champions': 3,\n",
       " 'charisma': 3,\n",
       " 'charismata': 3,\n",
       " 'charm': 2.5,\n",
       " 'charms': 2.5,\n",
       " 'cheer': 2.0,\n",
       " 'cheers': 2.0,\n",
       " 'commendation': 3,\n",
       " 'commendations': 3,\n",
       " 'creativity': 3,\n",
       " 'creativities': 3,\n",
       " 'dedication': 3,\n",
       " 'dedications': 3,\n",
       " 'delight': 3.0,\n",
       " 'delights': 3.0,\n",
       " 'devotion': 3,\n",
       " 'devotions': 3,\n",
       " 'divinity': 3,\n",
       " 'divinities': 3,\n",
       " 'eminence': 3,\n",
       " 'eminences': 3,\n",
       " 'enchantment': 3,\n",
       " 'enchantments': 3,\n",
       " 'enjoyment': 3,\n",
       " 'enjoyments': 3,\n",
       " 'exuberance': 3,\n",
       " 'exuberances': 3,\n",
       " 'exultation': 3,\n",
       " 'exultations': 3,\n",
       " 'faithfulness': 3,\n",
       " 'faithfulnesses': 3,\n",
       " 'fascination': 3,\n",
       " 'fascinations': 3,\n",
       " 'favorite': 3.5,\n",
       " 'favorites': 3,\n",
       " 'fulfillment': 3,\n",
       " 'fulfillments': 3,\n",
       " 'fun': 3.0,\n",
       " 'funs': 3,\n",
       " 'glee': 3,\n",
       " 'glees': 3,\n",
       " 'glory': 3,\n",
       " 'glories': 3,\n",
       " 'grace': 3,\n",
       " 'graces': 3,\n",
       " 'greatness': 3,\n",
       " 'greatnesses': 3,\n",
       " 'gusto': 3,\n",
       " 'gustoes': 3,\n",
       " 'heaven': 3,\n",
       " 'heavens': 3,\n",
       " 'humanity': 3,\n",
       " 'humanities': 3,\n",
       " 'ingenuity': 3,\n",
       " 'ingenuities': 3,\n",
       " 'insight': 3,\n",
       " 'insights': 3,\n",
       " 'inspiration': 3,\n",
       " 'inspirations': 3,\n",
       " 'joy': 3,\n",
       " 'joys': 3,\n",
       " 'loveliness': 3,\n",
       " 'lovelinesses': 3,\n",
       " 'majesty': 3,\n",
       " 'majesties': 3,\n",
       " 'marvel': 3.0,\n",
       " 'marvels': 3.0,\n",
       " 'milestone': 3,\n",
       " 'milestones': 3,\n",
       " 'nobility': 3,\n",
       " 'nobilities': 3,\n",
       " 'originality': 3,\n",
       " 'originalities': 3,\n",
       " 'panache': 3,\n",
       " 'panaches': 3,\n",
       " 'pleasure': 3,\n",
       " 'pleasures': 3,\n",
       " 'potency': 3,\n",
       " 'potencies': 3,\n",
       " 'prowess': 3,\n",
       " 'prowesses': 3,\n",
       " 'radiance': 3,\n",
       " 'radiances': 3,\n",
       " 'rapture': 3,\n",
       " 'raptures': 3,\n",
       " 'redeem': 3,\n",
       " 'redeems': 3,\n",
       " 'renaissance': 3,\n",
       " 'renaissances': 3,\n",
       " 'resourcefulness': 3,\n",
       " 'resourcefulnesses': 3,\n",
       " 'respect': 3.0,\n",
       " 'respects': 3.0,\n",
       " 'restoration': 3,\n",
       " 'restorations': 3,\n",
       " 'reverence': 3,\n",
       " 'reverences': 3,\n",
       " 'richness': 3,\n",
       " 'richnesses': 3,\n",
       " 'saint': 3,\n",
       " 'saints': 3,\n",
       " 'self-respect': 3,\n",
       " 'self-respects': 3,\n",
       " 'serenity': 3,\n",
       " 'serenities': 3,\n",
       " 'sincerity': 3,\n",
       " 'sincerities': 3,\n",
       " 'splendor': 3,\n",
       " 'splendors': 3,\n",
       " 'success': 3,\n",
       " 'successes': 3,\n",
       " 'sweetness': 3,\n",
       " 'sweetnesses': 3,\n",
       " 'talent': 3,\n",
       " 'talents': 3,\n",
       " 'thrill': 3.0,\n",
       " 'thrills': 3.0,\n",
       " 'triumph': 3.0,\n",
       " 'triumphs': 3.0,\n",
       " 'trustworthiness': 3,\n",
       " 'trustworthinesses': 3,\n",
       " 'value': 2.0,\n",
       " 'values': 2.0,\n",
       " 'versatility': 3,\n",
       " 'versatilities': 3,\n",
       " 'visionary': 3,\n",
       " 'visionaries': 3,\n",
       " 'zest': 3,\n",
       " 'zests': 3,\n",
       " 'merit': 2.5,\n",
       " 'merits': 2.5,\n",
       " 'cornucopia': 3,\n",
       " 'cornucopias': 3,\n",
       " 'honour': 3,\n",
       " 'honours': 3,\n",
       " 'epic': 3.5,\n",
       " 'epics': 3,\n",
       " 'renown': 3,\n",
       " 'renowns': 3,\n",
       " 'hilarity': 3,\n",
       " 'hilarities': 3,\n",
       " 'hilariousness': 3,\n",
       " 'hilariousnesses': 3,\n",
       " 'gumption': 3,\n",
       " 'gumptions': 3,\n",
       " 'delightfulness': 3,\n",
       " 'delightfulnesses': 3,\n",
       " 'awesomeness': 3,\n",
       " 'awesomenesses': 3,\n",
       " 'acumen': 3,\n",
       " 'acumens': 3,\n",
       " 'quality': 2.5,\n",
       " 'qualities': 2,\n",
       " 'aptitude': 2,\n",
       " 'aptitudes': 2,\n",
       " 'chivalry': 2,\n",
       " 'chivalries': 2,\n",
       " 'friend': 2,\n",
       " 'friends': 2,\n",
       " 'expert': 2,\n",
       " 'experts': 2,\n",
       " 'mercy': 2,\n",
       " 'mercies': 2,\n",
       " 'moral': 2.0,\n",
       " 'morals': 2,\n",
       " 'affirmation': 2,\n",
       " 'affirmations': 2,\n",
       " 'adventure': 2,\n",
       " 'adventures': 2,\n",
       " 'advantage': 2,\n",
       " 'advantages': 2,\n",
       " 'advancement': 2,\n",
       " 'advancements': 2,\n",
       " 'abundance': 2,\n",
       " 'abundances': 2,\n",
       " 'appreciation': 2,\n",
       " 'appreciations': 2,\n",
       " 'attraction': 2,\n",
       " 'attractions': 2,\n",
       " 'authenticity': 2,\n",
       " 'authenticities': 2,\n",
       " 'bargain': 2,\n",
       " 'bargains': 2,\n",
       " 'blockbuster': 2,\n",
       " 'blockbusters': 2,\n",
       " 'bonus': 2,\n",
       " 'bonuss': 2,\n",
       " 'boom': 2,\n",
       " 'booms': 2,\n",
       " 'boost': 2.0,\n",
       " 'boosts': 2.0,\n",
       " 'candor': 2,\n",
       " 'candors': 2,\n",
       " 'care': 2.0,\n",
       " 'cares': 2.0,\n",
       " 'capability': 2,\n",
       " 'capabilities': 2,\n",
       " 'charity': 2,\n",
       " 'charities': 2,\n",
       " 'civility': 2,\n",
       " 'civilities': 2,\n",
       " 'cooperation': 2.0,\n",
       " 'cooperations': 2.0,\n",
       " 'coordination': 2,\n",
       " 'coordinations': 2,\n",
       " 'cohesion': 2,\n",
       " 'cohesions': 2,\n",
       " 'collaboration': 2,\n",
       " 'collaborations': 2,\n",
       " 'commemoration': 2,\n",
       " 'commemorations': 2,\n",
       " 'commitment': 2,\n",
       " 'commitments': 2,\n",
       " 'compassion': 2,\n",
       " 'compassions': 2,\n",
       " 'competence': 2,\n",
       " 'competences': 2,\n",
       " 'compliment': 2.0,\n",
       " 'compliments': 2.0,\n",
       " 'confidence': 2,\n",
       " 'confidences': 2,\n",
       " 'congratulation': 2,\n",
       " 'congratulations': 2,\n",
       " 'conquest': 2,\n",
       " 'conquests': 2,\n",
       " 'conscience': 2,\n",
       " 'consciences': 2,\n",
       " 'consistency': 2,\n",
       " 'consistencies': 2,\n",
       " 'contentment': 2,\n",
       " 'contentments': 2,\n",
       " 'continuity': 2,\n",
       " 'continuities': 2,\n",
       " 'contribution': 2,\n",
       " 'contributions': 2,\n",
       " 'courage': 2,\n",
       " 'courages': 2,\n",
       " 'credibility': 2,\n",
       " 'credibilities': 2,\n",
       " 'decency': 2,\n",
       " 'decencies': 2,\n",
       " 'dependability': 2,\n",
       " 'dependabilities': 2,\n",
       " 'dexterity': 2,\n",
       " 'dexterities': 2,\n",
       " 'dignity': 2,\n",
       " 'dignities': 2,\n",
       " 'distinction': 2,\n",
       " 'distinctions': 2,\n",
       " 'dominance': 2,\n",
       " 'dominances': 2,\n",
       " 'durability': 2,\n",
       " 'durabilities': 2,\n",
       " 'earnestness': 2,\n",
       " 'earnestnesses': 2,\n",
       " 'effectiveness': 2,\n",
       " 'effectivenesses': 2,\n",
       " 'efficacy': 2,\n",
       " 'efficacies': 2,\n",
       " 'elegance': 2,\n",
       " 'elegances': 2,\n",
       " 'empathy': 2,\n",
       " 'empathies': 2,\n",
       " 'encouragement': 2,\n",
       " 'encouragements': 2,\n",
       " 'endorsement': 2,\n",
       " 'endorsements': 2,\n",
       " 'enhancement': 2,\n",
       " 'enhancements': 2,\n",
       " 'enrichment': 2,\n",
       " 'enrichments': 2,\n",
       " 'entertainment': 2,\n",
       " 'entertainments': 2,\n",
       " 'enthusiasm': 2,\n",
       " 'enthusiasms': 2,\n",
       " 'equality': 2,\n",
       " 'equalities': 2,\n",
       " 'excitement': 2,\n",
       " 'excitements': 2,\n",
       " 'fairness': 2,\n",
       " 'fairnesses': 2,\n",
       " 'faith': 2,\n",
       " 'faiths': 2,\n",
       " 'feast': 2,\n",
       " 'feasts': 2,\n",
       " 'feature': 2,\n",
       " 'features': 2,\n",
       " 'fellowship': 2,\n",
       " 'fellowships': 2,\n",
       " 'fervor': 2,\n",
       " 'fervors': 2,\n",
       " 'fidelity': 2,\n",
       " 'fidelities': 2,\n",
       " 'fitness': 2,\n",
       " 'fitnesses': 2,\n",
       " 'flair': 2,\n",
       " 'flairs': 2,\n",
       " 'foresight': 2,\n",
       " 'foresights': 2,\n",
       " 'fortune': 2,\n",
       " 'fortunes': 2,\n",
       " 'freedom': 2,\n",
       " 'freedoms': 2,\n",
       " 'friendship': 2,\n",
       " 'friendships': 2,\n",
       " 'fruition': 2,\n",
       " 'fruitions': 2,\n",
       " 'gain': 2.0,\n",
       " 'gains': 2.0,\n",
       " 'gallantry': 2,\n",
       " 'gallantries': 2,\n",
       " 'generosity': 2,\n",
       " 'generosities': 2,\n",
       " 'gift': 2,\n",
       " 'gifts': 2,\n",
       " 'goodwill': 2,\n",
       " 'goodwills': 2,\n",
       " 'goodness': 2,\n",
       " 'goodnesses': 2,\n",
       " 'gratitude': 2,\n",
       " 'gratitudes': 2,\n",
       " 'guarantee': 1.3333333333333333,\n",
       " 'guarantees': 1.3333333333333333,\n",
       " 'happiness': 2.0,\n",
       " 'harmony': 2,\n",
       " 'harmonies': 2,\n",
       " 'haven': 2,\n",
       " 'havens': 2,\n",
       " 'health': 2,\n",
       " 'healths': 2,\n",
       " 'hero': 2,\n",
       " 'heroes': 2,\n",
       " 'heroine': 2,\n",
       " 'heroines': 2,\n",
       " 'highlight': 2.0,\n",
       " 'highlights': 2.0,\n",
       " 'hit': 2.0,\n",
       " 'hits': 2,\n",
       " 'humor': 2,\n",
       " 'humors': 2,\n",
       " 'ideal': 3.0,\n",
       " 'ideals': 2,\n",
       " 'idealism': 2,\n",
       " 'idealisms': 2,\n",
       " 'imagination': 2,\n",
       " 'imaginations': 2,\n",
       " 'importance': 2,\n",
       " 'importances': 2,\n",
       " 'improvement': 2,\n",
       " 'improvements': 2,\n",
       " 'innocence': 2,\n",
       " 'innocences': 2,\n",
       " 'innovation': 2,\n",
       " 'innovations': 2,\n",
       " 'intensity': 2,\n",
       " 'intensities': 2,\n",
       " 'intrigue': 2.0,\n",
       " 'intrigues': 2.0,\n",
       " 'jubilee': 2,\n",
       " 'jubilees': 2,\n",
       " 'kindness': 2,\n",
       " 'kindnesses': 2,\n",
       " 'kudos': 2,\n",
       " 'kudoss': 2,\n",
       " 'laugh': 2.0,\n",
       " 'laughs': 2.0,\n",
       " 'lover': 2,\n",
       " 'lovers': 2,\n",
       " 'luxury': 2,\n",
       " 'luxuries': 2,\n",
       " 'master': 2.0,\n",
       " 'masters': 2.0,\n",
       " 'merriment': 2,\n",
       " 'merriments': 2,\n",
       " 'miracle': 2,\n",
       " 'miracles': 2,\n",
       " 'morality': 2,\n",
       " 'moralities': 2,\n",
       " 'notoriety': 2,\n",
       " 'notorieties': 2,\n",
       " 'paradise': 2,\n",
       " 'paradises': 2,\n",
       " 'passion': 2,\n",
       " 'passions': 2,\n",
       " 'perfectionist': 2,\n",
       " 'perfectionists': 2,\n",
       " 'perseverance': 2,\n",
       " 'perseverances': 2,\n",
       " 'popularity': 2,\n",
       " 'popularities': 2,\n",
       " 'positive': 2.5,\n",
       " 'positives': 2,\n",
       " 'praise': 2.5,\n",
       " 'praises': 2.5,\n",
       " 'prestige': 2,\n",
       " 'prestiges': 2,\n",
       " 'pride': 2,\n",
       " 'prides': 2,\n",
       " 'prodigy': 2,\n",
       " 'prodigies': 2,\n",
       " 'profit': 2.0,\n",
       " 'profits': 2.0,\n",
       " 'progress': 2.0,\n",
       " 'prominence': 2,\n",
       " 'prominences': 2,\n",
       " 'promise': 2,\n",
       " 'promises': 2,\n",
       " 'prosperity': 2,\n",
       " 'prosperities': 2,\n",
       " 'provocation': 2,\n",
       " 'provocations': 2,\n",
       " 'prudence': 2,\n",
       " 'prudences': 2,\n",
       " 'purity': 2,\n",
       " 'purities': 2,\n",
       " 'rapport': 2,\n",
       " 'rapports': 2,\n",
       " 'recommendation': 2,\n",
       " 'recommendations': 2,\n",
       " 'reconciliation': 2,\n",
       " 'reconciliations': 2,\n",
       " 'recreation': 2,\n",
       " 'recreations': 2,\n",
       " 'refinement': 2,\n",
       " 'refinements': 2,\n",
       " 'reliability': 2.0,\n",
       " 'reliabilities': 2.0,\n",
       " 'relief': 2,\n",
       " 'reliefs': 2,\n",
       " 'responsibility': 2,\n",
       " 'responsibilities': 2,\n",
       " 'reunion': 2,\n",
       " 'reunions': 2,\n",
       " 'reverent': 2.0,\n",
       " 'reverents': 2,\n",
       " 'reward': 2.0,\n",
       " 'rewards': 2.0,\n",
       " 'righteousness': 2,\n",
       " 'righteousnesses': 2,\n",
       " 'romance': 2,\n",
       " 'romances': 2,\n",
       " 'safety': 2,\n",
       " 'safeties': 2,\n",
       " 'sagacity': 2,\n",
       " 'sagacities': 2,\n",
       " 'sage': 2.5,\n",
       " 'sages': 2,\n",
       " 'sanctuary': 2,\n",
       " 'sanctuaries': 2,\n",
       " 'satisfaction': 2,\n",
       " 'satisfactions': 2,\n",
       " 'scruples': 2,\n",
       " 'scrupless': 2,\n",
       " 'security': 2,\n",
       " 'securities': 2,\n",
       " 'sensation': 2,\n",
       " 'sensations': 2,\n",
       " 'shrewdness': 2,\n",
       " 'shrewdnesses': 2,\n",
       " 'significance': 2,\n",
       " 'significances': 2,\n",
       " 'simplicity': 2,\n",
       " 'simplicities': 2,\n",
       " 'silliness': 2,\n",
       " 'sillinesses': 2,\n",
       " 'skill': 2,\n",
       " 'skills': 2,\n",
       " 'softness': 2,\n",
       " 'softnesses': 2,\n",
       " 'solution': 2,\n",
       " 'solutions': 2,\n",
       " 'standout': 2,\n",
       " 'standouts': 2,\n",
       " 'superiority': 2,\n",
       " 'superiorities': 2,\n",
       " 'surge': 2,\n",
       " 'surges': 2,\n",
       " 'sweetheart': 2,\n",
       " 'sweethearts': 2,\n",
       " 'swiftness': 2,\n",
       " 'swiftnesses': 2,\n",
       " 'sympathy': 2,\n",
       " 'sympathies': 2,\n",
       " 'tact': 2,\n",
       " 'tacts': 2,\n",
       " 'taste': 2,\n",
       " 'tastes': 2,\n",
       " 'tenacity': 2,\n",
       " 'tenacities': 2,\n",
       " 'tenderness': 2,\n",
       " 'tendernesses': 2,\n",
       " 'tolerance': 2,\n",
       " 'tolerances': 2,\n",
       " 'tranquility': 2,\n",
       " 'tranquilities': 2,\n",
       " 'treat': 2,\n",
       " 'treats': 2,\n",
       " 'tribute': 1.5,\n",
       " 'tributes': 1.5,\n",
       " 'uniqueness': 2,\n",
       " 'uniquenesses': 2,\n",
       " 'unity': 2,\n",
       " 'unities': 2,\n",
       " 'vastness': 2,\n",
       " 'vastnesses': 2,\n",
       " 'virtue': 2,\n",
       " 'virtues': 2,\n",
       " 'vitality': 2,\n",
       " 'vitalities': 2,\n",
       " 'warmth': 2,\n",
       " 'warmths': 2,\n",
       " 'worth': 1.6666666666666667,\n",
       " 'worths': 2.0,\n",
       " 'gem': 2,\n",
       " 'gems': 2,\n",
       " 'depth': 2,\n",
       " 'depths': 2,\n",
       " 'goddess': 2,\n",
       " 'goddesses': 2,\n",
       " 'stature': 2,\n",
       " 'statures': 2,\n",
       " 'strength': 2,\n",
       " 'strengths': 2,\n",
       " 'smash': 0.5,\n",
       " 'smashes': 0.5,\n",
       " 'pioneer': 2.0,\n",
       " 'pioneers': 2.0,\n",
       " 'glamour': 2,\n",
       " 'glamours': 2,\n",
       " 'best-seller': 2,\n",
       " 'best-sellers': 2,\n",
       " 'wit': 2,\n",
       " 'wits': 2,\n",
       " 'fan': 2,\n",
       " 'fans': 2,\n",
       " 'jewel': 2,\n",
       " 'jewels': 2,\n",
       " 'blessing': 2,\n",
       " 'blessings': 2,\n",
       " 'resurgence': 2,\n",
       " 'resurgences': 2,\n",
       " 'experise': 2,\n",
       " 'experises': 2,\n",
       " 'integrity': 2,\n",
       " 'integrities': 2,\n",
       " 'cinch': 2,\n",
       " 'cinches': 2,\n",
       " 'loyalty': 2.0,\n",
       " 'loyalties': 2.0,\n",
       " 'poignancy': 2,\n",
       " 'poignancies': 2,\n",
       " 'feat': 2,\n",
       " 'feats': 2,\n",
       " 'stand-out': 1.5,\n",
       " 'stands-out': 2,\n",
       " 'lyricism': 2,\n",
       " 'lyricisms': 2,\n",
       " 'inventiveness': 2,\n",
       " 'inventivenesses': 2,\n",
       " 'well-being': 2,\n",
       " 'well-beings': 2,\n",
       " 'honesty': 2,\n",
       " 'honesties': 2,\n",
       " 'friendliness': 2,\n",
       " 'friendlinesses': 2,\n",
       " 'graciousness': 2,\n",
       " 'graciousnesses': 2,\n",
       " 'felicity': 2,\n",
       " 'felicities': 2,\n",
       " 'eloquence': 2,\n",
       " 'eloquences': 2,\n",
       " 'elan': 2,\n",
       " 'elans': 2,\n",
       " 'courageousness': 2,\n",
       " 'courageousnesses': 2,\n",
       " 'accolade': 2,\n",
       " 'accolades': 2,\n",
       " 'victory': 2,\n",
       " 'victories': 2,\n",
       " 'strides': 2,\n",
       " 'stridess': 2,\n",
       " 'breakthrough': 2,\n",
       " 'breakthroughs': 2,\n",
       " 'interest': 1.5,\n",
       " 'interests': 1.5,\n",
       " 'acceptance': 1,\n",
       " 'acceptances': 1,\n",
       " 'ability': 1,\n",
       " 'abilities': 1,\n",
       " 'artist': 1,\n",
       " 'artists': 1,\n",
       " 'aspiration': 1,\n",
       " 'aspirations': 1,\n",
       " 'assistance': 1,\n",
       " 'assistances': 1,\n",
       " 'attune': 1,\n",
       " 'attunes': 1,\n",
       " 'augment': 1,\n",
       " 'augments': 1,\n",
       " 'beacon': 1,\n",
       " 'beacons': 1,\n",
       " 'brotherhood': 1,\n",
       " 'brotherhoods': 1,\n",
       " 'buzz': 1,\n",
       " 'buzzs': 1,\n",
       " 'calm': 1.5,\n",
       " 'calms': 1,\n",
       " 'captive': 1,\n",
       " 'captives': 1,\n",
       " 'cheerfulness': 1,\n",
       " 'cheerfulnesses': 1,\n",
       " 'chum': 1,\n",
       " 'chums': 1,\n",
       " 'clarity': 1,\n",
       " 'clarities': 1,\n",
       " 'cleanliness': 1,\n",
       " 'cleanlinesses': 1,\n",
       " 'clearness': 1,\n",
       " 'clearnesses': 1,\n",
       " 'clout': 1,\n",
       " 'clouts': 1,\n",
       " 'closeness': 1,\n",
       " 'closenesses': 1,\n",
       " 'comedy': 1,\n",
       " 'comedies': 1,\n",
       " 'comfort': 1.0,\n",
       " 'comforts': 1.0,\n",
       " 'community': 1,\n",
       " 'communities': 1,\n",
       " 'companion': 1,\n",
       " 'companions': 1,\n",
       " 'companionship': 1,\n",
       " 'companionships': 1,\n",
       " 'compensation': 1,\n",
       " 'compensations': 1,\n",
       " 'compromise': 1.0,\n",
       " 'compromises': 1.0,\n",
       " 'consent': 1,\n",
       " 'consents': 1,\n",
       " 'content': 1.0,\n",
       " 'contents': 1,\n",
       " 'conviction': 1,\n",
       " 'convictions': 1,\n",
       " 'correction': 1,\n",
       " 'corrections': 1,\n",
       " 'courtesy': 1,\n",
       " 'courtesies': 1,\n",
       " 'credential': 1,\n",
       " 'credentials': 1,\n",
       " 'credit': 1.5,\n",
       " 'credits': 1.5,\n",
       " 'crusade': 1.0,\n",
       " 'crusades': 1.0,\n",
       " 'culture': 1,\n",
       " 'cultures': 1,\n",
       " 'determination': 1,\n",
       " 'determinations': 1,\n",
       " 'diligence': 1,\n",
       " 'diligences': 1,\n",
       " 'discount': 1,\n",
       " 'discounts': 1,\n",
       " 'donation': 1,\n",
       " 'donations': 1,\n",
       " 'dream': 1,\n",
       " 'dreams': 1,\n",
       " 'eagerness': 1,\n",
       " 'eagernesses': 1,\n",
       " 'ease': 1,\n",
       " 'eases': 1,\n",
       " 'elaboration': 1,\n",
       " 'elaborations': 1,\n",
       " 'empowerment': 1,\n",
       " 'empowerments': 1,\n",
       " 'energy': 1,\n",
       " 'energies': 1,\n",
       " 'enlightenment': 1,\n",
       " 'enlightenments': 1,\n",
       " 'ethic': 1,\n",
       " 'ethics': 1,\n",
       " 'etiquette': 1,\n",
       " 'etiquettes': 1,\n",
       " 'excitedness': 1,\n",
       " 'excitednesses': 1,\n",
       " 'familiarity': 1,\n",
       " 'familiarities': 1,\n",
       " 'fantasy': 1,\n",
       " 'fantasies': 1,\n",
       " 'favor': 1.5,\n",
       " 'favors': 1.5,\n",
       " 'festivity': 1,\n",
       " 'festivities': 1,\n",
       " 'firmness': 1,\n",
       " 'firmnesses': 1,\n",
       " 'fondness': 1,\n",
       " 'fondnesses': 1,\n",
       " 'forceful': 1.5,\n",
       " 'forcefuls': 1,\n",
       " 'force': -0.5,\n",
       " 'forces': -0.5,\n",
       " 'forgiveness': 1,\n",
       " 'forgivenesses': 1,\n",
       " 'fortitude': 1,\n",
       " 'fortitudes': 1,\n",
       " 'gag': 1,\n",
       " 'gags': 1,\n",
       " 'gladness': 1,\n",
       " 'gladnesses': 1,\n",
       " 'gleam': 1.0,\n",
       " 'gleams': 1.0,\n",
       " 'glitter': 1.0,\n",
       " 'glitters': 1.0,\n",
       " 'gratification': 1,\n",
       " 'gratifications': 1,\n",
       " 'guardian': 1,\n",
       " 'guardians': 1,\n",
       " 'heart': 1,\n",
       " 'hearts': 1,\n",
       " 'helper': 1,\n",
       " 'helpers': 1,\n",
       " 'homage': 1,\n",
       " 'homages': 1,\n",
       " 'hope': 1,\n",
       " 'hopes': 1,\n",
       " 'impartiality': 1,\n",
       " 'impartialities': 1,\n",
       " 'impetus': 1,\n",
       " 'impetus ': 1,\n",
       " 'impression': 1,\n",
       " 'impressions': 1,\n",
       " 'independence': 1,\n",
       " 'independences': 1,\n",
       " 'indispensabilty': 1,\n",
       " 'indispensabilties': 1,\n",
       " 'intellect': 1,\n",
       " 'intellects': 1,\n",
       " 'intimacy': 1,\n",
       " 'intimacies': 1,\n",
       " 'jest': 1.0,\n",
       " 'jests': 1.0,\n",
       " 'joke': 1,\n",
       " 'jokes': 1,\n",
       " 'justice': 1,\n",
       " 'justices': 1,\n",
       " 'laughter': 1,\n",
       " 'laughters': 1,\n",
       " 'legitmacy': 1,\n",
       " 'legitmacies': 1,\n",
       " 'logic': 1,\n",
       " 'logics': 1,\n",
       " 'luck': 1,\n",
       " 'lucks': 1,\n",
       " 'luster': 1,\n",
       " 'lusters': 1,\n",
       " 'magic': 1,\n",
       " 'magics': 1,\n",
       " 'maturity': 1,\n",
       " 'maturities': 1,\n",
       " 'meditation': 1,\n",
       " 'meditations': 1,\n",
       " 'mirth': 1,\n",
       " 'mirths': 1,\n",
       " 'moderation': 1,\n",
       " 'moderations': 1,\n",
       " 'modernity': 1,\n",
       " 'modernities': 1,\n",
       " 'modesty': 1,\n",
       " 'modesties': 1,\n",
       " 'morale': 1,\n",
       " 'morales': 1,\n",
       " 'motivation': 1,\n",
       " 'motivations': 1,\n",
       " 'multitude': 1,\n",
       " 'multitudes': 1,\n",
       " 'nourishment': 1,\n",
       " 'nourishments': 1,\n",
       " 'novelty': 1,\n",
       " 'novelties': 1,\n",
       " 'nutrient': 1,\n",
       " 'nutrients': 1,\n",
       " 'obedience': 1,\n",
       " 'obediences': 1,\n",
       " 'opportunity': 1,\n",
       " 'opportunities': 1,\n",
       " 'optimism': 1,\n",
       " 'optimisms': 1,\n",
       " 'partner': 1,\n",
       " 'partners': 1,\n",
       " 'partnership': 1,\n",
       " 'partnerships': 1,\n",
       " 'patience': 1,\n",
       " 'patiences': 1,\n",
       " 'patriot': 1,\n",
       " 'patriots': 1,\n",
       " 'peace': 1,\n",
       " 'peaces': 1,\n",
       " 'perfectionism': 1,\n",
       " 'perfectionisms': 1,\n",
       " 'plausibility': 1,\n",
       " 'plausibilities': 1,\n",
       " 'positiveness': 1,\n",
       " 'positivenesses': 1,\n",
       " 'positivity': 1,\n",
       " 'positivities': 1,\n",
       " 'posterity': 1,\n",
       " 'posterities': 1,\n",
       " 'precaution': 1,\n",
       " 'precautions': 1,\n",
       " 'precedent': 1,\n",
       " 'precedents': 1,\n",
       " 'prize': 2.0,\n",
       " 'prizes': 2.0,\n",
       " 'pro': 1,\n",
       " 'pros': 1,\n",
       " 'productivity': 1,\n",
       " 'productivities': 1,\n",
       " 'professional': 2.0,\n",
       " 'professionals': 1,\n",
       " 'protection': 1,\n",
       " 'protections': 1,\n",
       " 'purification': 1,\n",
       " 'purifications': 1,\n",
       " 'reassurance': 1,\n",
       " 'reassurances': 1,\n",
       " 'reinforcement': 1,\n",
       " 'reinforcements': 1,\n",
       " 'relevancy': 1,\n",
       " 'relevancies': 1,\n",
       " 'remedy': 1,\n",
       " 'remedies': 1,\n",
       " 'renovation': 1,\n",
       " 'renovations': 1,\n",
       " 'repentance': 1,\n",
       " 'repentances': 1,\n",
       " 'respite': 1,\n",
       " 'respites': 1,\n",
       " 'resurrect': 1,\n",
       " 'resurrects': 1,\n",
       " 'revival': 1,\n",
       " 'revivals': 1,\n",
       " 'riches': 1,\n",
       " 'richess': 1,\n",
       " 'righteous': -0.5,\n",
       " 'righteouss': 1,\n",
       " 'salutation': 1,\n",
       " 'salutations': 1,\n",
       " 'salvation': 1,\n",
       " 'salvations': 1,\n",
       " 'sanctity': 1,\n",
       " 'sanctities': 1,\n",
       " 'sanity': 1,\n",
       " 'sanities': 1,\n",
       " 'semblance': 1,\n",
       " 'semblances': 1,\n",
       " 'sensitivity': 1,\n",
       " 'sensitivities': 1,\n",
       " 'seriousness': 1,\n",
       " 'seriousnesses': 1,\n",
       " 'shelter': 1,\n",
       " 'shelters': 1,\n",
       " 'smile': 1.0,\n",
       " 'smiles': 1.0,\n",
       " 'solace': 1,\n",
       " 'solaces': 1,\n",
       " 'spell': 1,\n",
       " 'spells': 1,\n",
       " 'stability': 1,\n",
       " 'stabilities': 1,\n",
       " 'staunchness': 1,\n",
       " 'staunchnesses': 1,\n",
       " 'steadiness': 1,\n",
       " 'steadinesses': 1,\n",
       " 'steadfastness': 1,\n",
       " 'steadfastnesses': 1,\n",
       " 'stimulation': 1,\n",
       " 'stimulations': 1,\n",
       " 'substance': 1,\n",
       " 'substances': 1,\n",
       " 'synthesis': 1,\n",
       " 'syntheses': 1,\n",
       " 'temperance': 1,\n",
       " 'temperances': 1,\n",
       " 'thrift': 1,\n",
       " 'thrifts': 1,\n",
       " 'toleration': 1,\n",
       " 'tolerations': 1,\n",
       " 'trophy': 1,\n",
       " 'trophies': 1,\n",
       " 'truth': 1,\n",
       " 'truths': 1,\n",
       " 'trust': 1.5,\n",
       " 'trusts': 1.5,\n",
       " 'understanding': 1.0,\n",
       " 'usefulness': 1,\n",
       " 'usefulnesses': 1,\n",
       " 'validity': 1,\n",
       " 'validities': 1,\n",
       " 'variety': 1,\n",
       " 'varieties': 1,\n",
       " 'viability': 1,\n",
       " 'viabilities': 1,\n",
       " 'welfare': 1,\n",
       " 'welfares': 1,\n",
       " 'willingness': 1,\n",
       " 'willingnesses': 1,\n",
       " 'humility': 1,\n",
       " 'humilities': 1,\n",
       " 'toughness': 1,\n",
       " 'toughnesses': 1,\n",
       " 'helpfulness': 1,\n",
       " 'helpfulnesses': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement fix\n",
    "fix_lex(socallex, socallex_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced a_mutltidue_of by a_multitude_of\n",
      "Replaced visable by visible\n",
      "Replaced collossal by colossal\n"
     ]
    }
   ],
   "source": [
    "# Fix the intensifier/modifier dictionary the same way\n",
    "\n",
    "socalmods_fix = {\n",
    "    \"a_mutltidue_of\": \"a_multitude_of\",\n",
    "    \"visable\": \"visible\",\n",
    "    \"collossal\": \"colossal\",\n",
    "}\n",
    "\n",
    "fix_lex(socalmods, socalmods_fix)\n",
    "\n",
    "# Change one entry\n",
    "socalmods[\"more\"] = 0.5  # was -0.5 (i.e a weakening rather than a strengthening)\n",
    "\n",
    "# Add some additional entries\n",
    "socalmods[\"absence_of\"] = -1.5  # Negater, not in SO-CAL lexicon\n",
    "socalmods[\"devoid_of\"] = -1.5  # ,,\n",
    "socalmods[\"lack_of\"] = -1.5  # ,,\n",
    "socalmods[\"not_very\"] = -1.5  # Addition to parallel 'not_too'\n",
    "\n",
    "# Add some intensifiers from an earlier version of SO-CAL\n",
    "socalmods[\"low\"] = -2.0\n",
    "socalmods[\"some\"] = -0.2\n",
    "socalmods[\"obvious\"] = 0.3\n",
    "socalmods[\"lots_of\"] = 0.3\n",
    "socalmods[\"serious\"] = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated versions. Add an 'X' to indicate they're modified\n",
    "\n",
    "with open(socalfolder + \"SO-CAL_lexiconX.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(socallex.items())))\n",
    "\n",
    "with open(socalfolder + \"SO-CAL_modifiersX.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(socalmods.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. WordStat\n",
    "\n",
    "Provalis Research, the makers of WordStat, make available for public use a basic sentiment dictionary. This dictionary used to be located at http://www.provalisresearch.com/wordstat/Sentiment-Analysis.html (URL no longer live, but probably accessible via Wayback Machine), and that is the version we used through 2021. Our current set-up uses the newer version.\n",
    "\n",
    "The new version (2.0, dated 2018) is at https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/sentiment-dictionaries/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../MultiLexScaledCorpora/Lexica/English/MultiLexScaled/WordStat/WSD 2.0/'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE directory\n",
    "os.makedirs(f\"{SAfolder}WordStat/WSD 2.0/\", exist_ok=True)\n",
    "\n",
    "wordstatfolder = SAfolder + \"WordStat/WSD 2.0/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importWordStat(WSfolder, WSname, savepickle=False, savetext=True):\n",
    "    \"\"\"Import WordStat sentiment dictionary.\n",
    "\n",
    "    The dictionary file begins with several good/bad expressions; skip these because our\n",
    "    modifiers take care of them. This is followed by a list of negations (again, skip)\n",
    "    and a list of \"double negations\" (again, skip).\n",
    "\n",
    "    Finally, there are two sections titled \"NEGATIVE WORDS\" and \"POSITIVE WORDS\".\n",
    "    These we load into the WordStat dictionary.\n",
    "\n",
    "    The file is all caps, so make sure to lower-case things.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    import pickle\n",
    "\n",
    "    words = {}\n",
    "    poswords, negwords = 0, 0\n",
    "    negationwords, doublenegwords = [], []\n",
    "    curCat = \"prelims\"\n",
    "\n",
    "    with open(WSfolder + WSname, \"r\", errors=\"ignore\") as infile:\n",
    "        for aWord in infile.readlines():\n",
    "            aWord = aWord.lower().strip()\n",
    "            if aWord == \"negations\":\n",
    "                curCat = \"negs\"\n",
    "            elif aWord == \"double_negation\":\n",
    "                curCat = \"doublenegs\"\n",
    "            elif aWord == \"positive words\":\n",
    "                curCat = \"positive\"\n",
    "            elif aWord == \"negative words\":\n",
    "                curCat = \"negative\"\n",
    "            elif aWord == \"exceptions\":\n",
    "                curCat = \"exceptions\"\n",
    "            elif curCat in (\"positive\", \"negative\", \"negs\", \"doublenegs\"):\n",
    "                aWord = aWord[:-4]  # remove ' (1)' at end of each line\n",
    "                if (\n",
    "                    curCat == \"negs\"\n",
    "                ):  # We don't actually do anything with negs and doublenegs, so could skip this\n",
    "                    negationwords.append(aWord)\n",
    "                elif curCat == \"doublenegs\":\n",
    "                    doublenegwords.append(aWord.replace(\"_\", \" \"))\n",
    "                elif curCat == \"positive\":\n",
    "                    words[aWord] = 1\n",
    "                    poswords += 1\n",
    "                else:  # curCat == 'negative'\n",
    "                    words[aWord] = -1\n",
    "                    negwords += 1\n",
    "\n",
    "    print(\n",
    "        \"Dictionary contained {} positive and {} negative words (including wildcards) for a total of {} words\".format(\n",
    "            poswords, negwords, len(words)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if savepickle:\n",
    "        # Optionally add in the negationwords and doublenegwords here\n",
    "        with open(\"WordStatDictionary.pkl\", \"wb\") as outfile:\n",
    "            pickle.dump(words, outfile)\n",
    "    if savetext:\n",
    "        with open(WSfolder + \"WordStat_lexicon.csv\", \"w\") as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary contained 4710 positive and 9579 negative words (including wildcards) for a total of 14289 words\n"
     ]
    }
   ],
   "source": [
    "# Load WordStat lexicon (version 2.0)\n",
    "wordstatfolder = SAfolder + \"WordStat/WSD 2.0/\"\n",
    "wordstatfile = \"WordStat Sentiments.cat\"\n",
    "\n",
    "wslex2 = importWordStat(wordstatfolder, wordstatfile, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 258 entries with underscores (phrases)\n"
     ]
    }
   ],
   "source": [
    "# 2.0 has phrases, with underscores; remove these\n",
    "keys2delete = []\n",
    "for key, val in wslex2.items():\n",
    "    if \"_\" in key:\n",
    "        keys2delete.append(key)\n",
    "print(\"Deleting {} entries with underscores (phrases)\".format(len(keys2delete)))\n",
    "for key in keys2delete:\n",
    "    del wslex2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 4 entries starting with @ (comments/notes)\n"
     ]
    }
   ],
   "source": [
    "# It also has a few lines beginning with @ that are comments; remove these too\n",
    "keys2delete = []\n",
    "for key, val in wslex2.items():\n",
    "    if key[0] == \"@\":\n",
    "        keys2delete.append(key)\n",
    "print(\"Deleting {} entries starting with @ (comments/notes)\".format(len(keys2delete)))\n",
    "for key in keys2delete:\n",
    "    del wslex2[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abus* subsumes abuse\n",
      "aggravat* subsumes aggravate\n",
      "aggress* subsumes aggressive\n",
      "agoniz* subsumes agonizing\n",
      "alarm* subsumes alarm\n",
      "annoy* subsumes annoy\n",
      "annoy* subsumes annoyed\n",
      "annoy* subsumes annoying\n",
      "appall* subsumes appalling\n",
      "arrogan* subsumes arroganc*\n",
      "arrogant* subsumes arrogant\n",
      "arrogan* subsumes arrogant*\n",
      "attack* subsumes attacked\n",
      "avoid* subsumes avoid\n",
      "awe* subsumes aweful\n",
      "awe* and aweful have different valences => keeping both\n",
      "awkward* subsumes awkward\n",
      "bitter* subsumes bitter\n",
      "blam* subsumes blame\n",
      "bore* subsumes bore\n",
      "bother* subsumes bother\n",
      "bother* subsumes bothered\n",
      "bother* subsumes bothering\n",
      "bother* subsumes bothersome\n",
      "break* subsumes break\n",
      "brok* subsumes broke\n",
      "brok* subsumes broken\n",
      "brutal* subsumes brutal\n",
      "bullshit* subsumes bullshit\n",
      "burden* subsumes burden\n",
      "burden* subsumes burdensome\n",
      "challeng* subsumes challenging\n",
      "challeng* and challenging have different valences => keeping both\n",
      "chao* subsumes chaotic\n",
      "cheat* subsumes cheat\n",
      "cheat* subsumes cheated\n",
      "complain* subsumes complain\n",
      "complain* subsumes complained\n",
      "complain* subsumes complaining\n",
      "complain* subsumes complaint\n",
      "complain* subsumes complaints\n",
      "confus* subsumes confused\n",
      "confus* subsumes confusing\n",
      "confus* subsumes confusion\n",
      "craz* subsumes crazy\n",
      "crowd* subsumes crowded\n",
      "damag* subsumes damage\n",
      "damag* subsumes damaged\n",
      "danger* subsumes danger\n",
      "danger* subsumes dangerous\n",
      "dead* subsumes dead\n",
      "death* subsumes death\n",
      "decei* subsumes deceive\n",
      "demolish* subsumes demolish\n",
      "depres* subsumes depressing\n",
      "desperat* subsumes desperately\n",
      "devil* subsumes devil\n",
      "difficult* subsumes difficult\n",
      "disappoint* subsumes disappointed\n",
      "disappoint* subsumes disappointing\n",
      "disappoint* subsumes disappointment\n",
      "disaster* subsumes disaster\n",
      "discomfort* subsumes discomfort\n",
      "disgust* subsumes disgusted\n",
      "disgust* subsumes disgusting\n",
      "disgust* subsumes disgustingly\n",
      "disput* subsumes dispute\n",
      "dissatisf* subsumes dissatisfied\n",
      "distres* subsumes distressing\n",
      "disturb* subsumes disturbed\n",
      "disturb* subsumes disturbing\n",
      "doubt* subsumes doubt\n",
      "dread* subsumes dreadful\n",
      "dreary* subsumes dreary\n",
      "dull* subsumes dull\n",
      "dump* subsumes dump\n",
      "dying* subsumes dying\n",
      "decay* subsumes decay\n",
      "despair* subsumes despair\n",
      "desperat* subsumes desperate\n",
      "destroy* subsumes destroy\n",
      "dislik* subsumes dislike\n",
      "distres* subsumes distress\n",
      "disturb* subsumes disturb\n",
      "empt* subsumes empty\n",
      "fail* subsumes fail\n",
      "fail* subsumes failed\n",
      "fail* subsumes failure\n",
      "fault* subsumes fault\n",
      "fight* subsumes fight\n",
      "fight* subsumes fighting\n",
      "fool* subsumes fool\n",
      "fool* subsumes fooled\n",
      "forbid* subsumes forbid\n",
      "frustrat* subsumes frustrated\n",
      "frustrat* subsumes frustrating\n",
      "furious* subsumes furious\n",
      "fury* subsumes fury\n",
      "gloom* subsumes gloomy\n",
      "grave* subsumes grave\n",
      "greed* subsumes greed\n",
      "grie* subsumes grief\n",
      "grim* subsumes grim\n",
      "gross* subsumes gross\n",
      "growl* subsumes growl\n",
      "harass* subsumes harass\n",
      "harass* subsumes harassed\n",
      "harsh* subsumes harsh\n",
      "hate* subsumes hate\n",
      "hate* subsumes hated\n",
      "horr* subsumes horrendous\n",
      "horr* subsumes horrible\n",
      "horr* subsumes horribly\n",
      "horr* subsumes horrid\n",
      "horr* subsumes horrified\n",
      "horr* subsumes horror\n",
      "hostil* subsumes hostile\n",
      "hurt* subsumes hurt\n",
      "ignor* subsumes ignored\n",
      "impatien* subsumes impatient\n",
      "inadequa* subsumes inadequate\n",
      "inferior* subsumes inferior\n",
      "interrup* subsumes interrupt\n",
      "interrup* subsumes interrupted\n",
      "irrita* subsumes irritated\n",
      "irrita* subsumes irritating\n",
      "kill* subsumes killed\n",
      "lame* subsumes lame\n",
      "lazy* subsumes lazy\n",
      "loss* subsumes loss\n",
      "lous* subsumes lousy\n",
      "low* subsumes lower\n",
      "low* subsumes lowest\n",
      "miser* subsumes miserable\n",
      "mistak* subsumes mistake\n",
      "mistak* subsumes mistaken\n",
      "nast* subsumes nasty\n",
      "neglect* subsumes neglect\n",
      "neglect* subsumes neglected\n",
      "neglect* subsumes neglecting\n",
      "obnoxious* subsumes obnoxious\n",
      "odd* subsumes odd\n",
      "offend* subsumes offend\n",
      "offend* subsumes offended\n",
      "offens* subsumes offensive\n",
      "oppos* subsumes opposed\n",
      "outrag* subsumes outrageous\n",
      "outrag* subsumes outrageously\n",
      "outstand* subsumes outstanding\n",
      "outstand* and outstanding have different valences => keeping both\n",
      "outrag* subsumes outrage\n",
      "overwhelm* subsumes overwhelming\n",
      "pain* subsumes pain\n",
      "perver* subsumes perversion\n",
      "piti* subsumes pitiful\n",
      "pity* subsumes pity\n",
      "poison* subsumes poisoning\n",
      "predatory* subsumes predatory\n",
      "pressur* subsumes pressure\n",
      "pressur* subsumes pressured\n",
      "prick* subsumes prickly\n",
      "problem* subsumes problem\n",
      "problem* subsumes problems\n",
      "protest* subsumes protest\n",
      "punish* subsumes punishment\n",
      "regret* subsumes regretted\n",
      "relentles* subsumes relentless\n",
      "relentles* subsumes relentlessly\n",
      "revolt* subsumes revolt\n",
      "ridicul* subsumes ridiculous\n",
      "ripp* subsumes ripped\n",
      "risk* subsumes risk\n",
      "rude* subsumes rude\n",
      "ruin* subsumes ruin\n",
      "ruin* subsumes ruined\n",
      "savage* subsumes savage\n",
      "scare* subsumes scared\n",
      "sever* subsumes severe\n",
      "shame* subsumes shame\n",
      "shit* subsumes shit\n",
      "shock* subsumes shock\n",
      "shock* subsumes shocked\n",
      "shock* subsumes shocking\n",
      "sicken* subsumes sickening\n",
      "slap* subsumes slap\n",
      "snob* subsumes snobby\n",
      "sorry* subsumes sorry\n",
      "steal* subsumes stealing\n",
      "stench* subsumes stench\n",
      "stink* subsumes stink\n",
      "stink* subsumes stinks\n",
      "stol* subsumes stole\n",
      "stol* subsumes stolen\n",
      "strain* subsumes strain\n",
      "strang* subsumes strange\n",
      "stress* subsumes stress\n",
      "stress* subsumes stressed\n",
      "strik* subsumes strike\n",
      "stupid* subsumes stupid\n",
      "suffer* subsumes suffer\n",
      "terribl* subsumes terrible\n",
      "terribl* subsumes terribly\n",
      "terror* subsumes terror\n",
      "torn* subsumes torn\n",
      "tough* subsumes tough\n",
      "trick* subsumes trick\n",
      "tortur* subsumes torture\n",
      "troubl* subsumes trouble\n",
      "ugl* subsumes ugly\n",
      "uncomfortabl* subsumes uncomfortable\n",
      "unfortunate* subsumes unfortunately\n",
      "unhapp* subsumes unhappy\n",
      "unimpress* subsumes unimpressed\n",
      "unkind* subsumes unkind\n",
      "upset* subsumes upset\n",
      "useless* subsumes useless\n",
      "weak* subsumes weak\n",
      "weak* subsumes weaken\n",
      "weird* subsumes weird\n",
      "wept* subsumes wept\n",
      "worr* subsumes worry\n",
      "worse* subsumes worse\n",
      "worthless* subsumes worthless\n",
      "wound* subsumes wound\n",
      "wreck* subsumes wreck\n",
      "wrong* subsumes wrong\n",
      "accus* subsumes accustomed\n",
      "accus* and accustomed have different valences => keeping both\n",
      "blam* subsumes blameless\n",
      "blam* and blameless have different valences => keeping both\n",
      "boundles* subsumes boundless\n",
      "boundles* and boundless have different valences => keeping both\n",
      "break* subsumes breakthrough\n",
      "break* and breakthrough have different valences => keeping both\n",
      "break* subsumes breakthroughs\n",
      "break* and breakthroughs have different valences => keeping both\n",
      "defens* subsumes defensible\n",
      "defens* and defensible have different valences => keeping both\n",
      "domina* subsumes dominate\n",
      "domina* and dominate have different valences => keeping both\n",
      "domina* subsumes dominates\n",
      "domina* and dominates have different valences => keeping both\n",
      "doubt* subsumes doubtless\n",
      "doubt* and doubtless have different valences => keeping both\n",
      "envy* subsumes envy\n",
      "envy* and envy have different valences => keeping both\n",
      "fault* subsumes faultless\n",
      "fault* and faultless have different valences => keeping both\n",
      "fool* subsumes foolproof\n",
      "fool* and foolproof have different valences => keeping both\n",
      "harmless* subsumes harmless\n",
      "kind* subsumes kind\n",
      "pain* subsumes painl*\n",
      "pain* and painl* have different valences => keeping both\n",
      "solemn* subsumes solemn\n",
      "solemn* and solemn have different valences => keeping both\n",
      "solemn* subsumes solemnity\n",
      "solemn* and solemnity have different valences => keeping both\n",
      "stab* subsumes stability\n",
      "stab* and stability have different valences => keeping both\n",
      "stab* subsumes stabilization\n",
      "stab* and stabilization have different valences => keeping both\n",
      "stab* subsumes stabilize\n",
      "stab* and stabilize have different valences => keeping both\n",
      "stab* subsumes stabilized\n",
      "stab* and stabilized have different valences => keeping both\n",
      "stab* subsumes stabilizes\n",
      "stab* and stabilizes have different valences => keeping both\n",
      "stab* subsumes stabilizing\n",
      "stab* and stabilizing have different valences => keeping both\n",
      "stab* subsumes stable\n",
      "stab* and stable have different valences => keeping both\n",
      "strik* subsumes striking\n",
      "strik* and striking have different valences => keeping both\n",
      "temper* subsumes temperance\n",
      "temper* and temperance have different valences => keeping both\n",
      "tough* subsumes tougher\n",
      "tough* and tougher have different valences => keeping both\n",
      "Found 222 subsumed entries; deleting now\n"
     ]
    }
   ],
   "source": [
    "# Filter out subsumptions (if they have the same valence) and save\n",
    "wslex2 = lex_removesubsumed(wslex2)\n",
    "with open(wordstatfolder + \"WordStat_lexicon2X.csv\", \"w\") as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(wslex2.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
